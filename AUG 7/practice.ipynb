{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5bb664e-e284-41eb-9d3e-3c91791e8019",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+---------+---------+\n|EmpID|  Name|Department|UnitsSold|UnitPrice|\n+-----+------+----------+---------+---------+\n| E101|  Amit|     Sales|       10|     1200|\n| E102| Sneha| Marketing|        8|     1500|\n| E103|  Ravi|     Sales|       12|     1300|\n| E104|Anjali|        HR|        7|     1100|\n| E105|   Raj|     Sales|        5|     1000|\n+-----+------+----------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"E101\", \"Amit\", \"Sales\", 10, 1200),\n",
    "    (\"E102\", \"Sneha\", \"Marketing\", 8, 1500),\n",
    "    (\"E103\", \"Ravi\", \"Sales\", 12, 1300),\n",
    "    (\"E104\", \"Anjali\", \"HR\", 7, 1100),\n",
    "    (\"E105\", \"Raj\", \"Sales\", 5, 1000)\n",
    "]\n",
    "columns = [\"EmpID\", \"Name\", \"Department\", \"UnitsSold\", \"UnitPrice\"]\n",
    "\n",
    "df = spark.createDataFrame (data, columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "808e2d79-3f4b-4122-896d-297404bb0caf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CSV\n",
    "df.write.mode(\"overwrite\").option(\"header\", True).csv(\"/tmp/employees_csv\")\n",
    "\n",
    "\n",
    "# JSON\n",
    "df.write.mode(\"overwrite\").json(\"/tmp/employees_json\")\n",
    "\n",
    "# Parquet\n",
    "df.write.mode(\"overwrite\").parquet(\"/tmp/employees_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01d4c258-a689-43fc-a15d-78184e1b6416",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+---------+---------+\n|EmpID|  Name|Department|UnitsSold|UnitPrice|\n+-----+------+----------+---------+---------+\n| E102| Sneha| Marketing|        8|     1500|\n| E101|  Amit|     Sales|       10|     1200|\n| E103|  Ravi|     Sales|       12|     1300|\n| E104|Anjali|        HR|        7|     1100|\n| E105|   Raj|     Sales|        5|     1000|\n+-----+------+----------+---------+---------+\n\n+----------+-----+------+---------+---------+\n|Department|EmpID|  Name|UnitPrice|UnitsSold|\n+----------+-----+------+---------+---------+\n| Marketing| E102| Sneha|     1500|        8|\n|     Sales| E101|  Amit|     1200|       10|\n|     Sales| E103|  Ravi|     1300|       12|\n|        HR| E104|Anjali|     1100|        7|\n|     Sales| E105|   Raj|     1000|        5|\n+----------+-----+------+---------+---------+\n\n+-----+------+----------+---------+---------+\n|EmpID|  Name|Department|UnitsSold|UnitPrice|\n+-----+------+----------+---------+---------+\n| E102| Sneha| Marketing|        8|     1500|\n| E101|  Amit|     Sales|       10|     1200|\n| E103|  Ravi|     Sales|       12|     1300|\n| E104|Anjali|        HR|        7|     1100|\n| E105|   Raj|     Sales|        5|     1000|\n+-----+------+----------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "csv_df=spark.read.option(\"header\", True).csv(\"/tmp/employees_csv\")\n",
    "csv_df.show()\n",
    "json_df=spark.read.json(\"/tmp/employees_json\")\n",
    "json_df.show()\n",
    "parquet_df=spark.read.parquet(\"/tmp/employees_parquet\")\n",
    "parquet_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-08-07 10:13:50",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}