{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d9f81ac-d476-4ed7-8d46-07862e6ea8e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pyspark==3.5.0 delta-spark==3.1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa108e89-6274-4a58-9be3-c10e7c08416e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "builder = (\n",
    "SparkSession.builder.appName(\"DeltaDemo\")\n",
    ".config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    ".config(\"spark.sql.catalog.spark_catalog\",\n",
    "\"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "# Create sample DataFrame\n",
    "data = [\n",
    "(1, \"John\", \"Electronics\", 2, 300),\n",
    "(2, \"Sara\", \"Clothing\", 1, 50),\n",
    "(3, \"Mike\", \"Electronics\", 4, 600),\n",
    "(4, \"Nina\", \"Clothing\", 3, 150),\n",
    "]\n",
    "columns = [\"order_id\", \"customer_name\", \"category\", \"quantity\", \"amount\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "# Save as Delta table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/orders_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "652de6b6-c9b4-4f93-abde-68be64fb03ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Managed Table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE orders_managed\n",
    "USING DELTA\n",
    "LOCATION '/tmp/orders_delta'\n",
    "\"\"\")\n",
    "\n",
    "# Create Unmanaged Table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE orders_unmanaged\n",
    "USING DELTA\n",
    "LOCATION '/tmp/orders_delta'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44ec4ef0-1d66-48cb-ab3b-f5424b134e35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read from the table\n",
    "df_orders = spark.read.format(\"delta\").load(\"/tmp/orders_delta\")\n",
    "df_orders.show()\n",
    "\n",
    "# Update: Increase amount for 'Clothing' category by 20\n",
    "spark.sql(\"\"\"\n",
    "UPDATE orders_managed\n",
    "SET amount = amount + 20\n",
    "WHERE category = 'Clothing'\n",
    "\"\"\")\n",
    "\n",
    "# Delete: Remove all orders with quantity < 2\n",
    "spark.sql(\"\"\"\n",
    "DELETE FROM orders_managed\n",
    "WHERE quantity < 2\n",
    "\"\"\")\n",
    "\n",
    "# Prepare new and updated orders\n",
    "new_data = [\n",
    "    (5, \"Anna\", \"Furniture\", 2, 400),\n",
    "    (6, \"James\", \"Clothing\", 1, 100),\n",
    "]\n",
    "new_columns = [\"order_id\", \"customer_name\", \"category\", \"quantity\", \"amount\"]\n",
    "new_df = spark.createDataFrame(new_data, new_columns)\n",
    "\n",
    "# Merge new data into the table\n",
    "new_df.write.format(\"delta\").mode(\"append\").save(\"/tmp/orders_delta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e81c27d-76fb-4a5e-a37e-4b0027ca9fc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Describe the history of the Delta table\n",
    "spark.sql(\"DESCRIBE HISTORY delta.`/tmp/orders_delta`\").show()\n",
    "\n",
    "# Read data from version 0 (initial version)\n",
    "df_version_0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/tmp/orders_delta\")\n",
    "df_version_0.show()\n",
    "\n",
    "# Read data from the latest version\n",
    "df_latest = spark.read.format(\"delta\").load(\"/tmp/orders_delta\")\n",
    "df_latest.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6517ef6e-70c2-4d54-bd85-3466297bd7ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# New batch with orders 5 and 6\n",
    "new_orders_batch = [\n",
    "    (5, \"Anna\", \"Furniture\", 2, 400),\n",
    "    (6, \"James\", \"Clothing\", 1, 100),\n",
    "]\n",
    "batch_columns = [\"order_id\", \"customer_name\", \"category\", \"quantity\", \"amount\"]\n",
    "new_batch_df = spark.createDataFrame(new_orders_batch, batch_columns)\n",
    "\n",
    "# Append the new batch data to the Delta table\n",
    "new_batch_df.write.format(\"delta\").mode(\"append\").save(\"/tmp/orders_delta\")\n",
    "\n",
    "# Show final table after the append\n",
    "final_df = spark.read.format(\"delta\").load(\"/tmp/orders_delta\")\n",
    "final_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df2ae8fa-f7d3-47e0-bc69-fbb5276e03f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read Delta table into Pandas DataFrame\n",
    "df_pandas = spark.read.format(\"delta\").load(\"/tmp/orders_delta\").toPandas()\n",
    "\n",
    "# Group by category and sum the amount\n",
    "category_amount = df_pandas.groupby('category')['amount'].sum().reset_index()\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(category_amount['category'], category_amount['amount'], color='skyblue')\n",
    "plt.title(\"Total Amount by Category\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Total Amount\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Azure Databricks Assessment 1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}