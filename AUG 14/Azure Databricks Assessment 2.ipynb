{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf6ad1f1-b274-4876-bafd-545959302621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "# Spark with Delta enabled (works in Colab & Databricks)\n",
    "builder = (\n",
    "    SparkSession.builder.appName(\"DeltaRideHailing\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# ---------- Inline Data ----------\n",
    "trip_schema = T.StructType([\n",
    "    T.StructField(\"trip_id\", T.IntegerType()),\n",
    "    T.StructField(\"rider_id\", T.StringType()),\n",
    "    T.StructField(\"driver_id\", T.StringType()),\n",
    "    T.StructField(\"city\", T.StringType()),\n",
    "    T.StructField(\"distance_km\", T.DoubleType()),\n",
    "    T.StructField(\"fare\", T.DoubleType()),\n",
    "    T.StructField(\"tip\", T.DoubleType()),\n",
    "    T.StructField(\"ts\", T.TimestampType())\n",
    "])\n",
    "\n",
    "driver_schema = T.StructType([\n",
    "    T.StructField(\"driver_id\", T.StringType()),\n",
    "    T.StructField(\"driver_name\", T.StringType()),\n",
    "    T.StructField(\"rating\", T.DoubleType()),\n",
    "    T.StructField(\"vehicle\", T.StringType())\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "trips_rows = [\n",
    "    (1001, \"R001\", \"D010\", \"Bengaluru\", 12.4, 320.0, 20.0, F.to_timestamp(F.lit(\"2025-08-08 08:05:00\"))),\n",
    "    (1002, \"R002\", \"D011\", \"Hyderabad\", 6.2, 150.0, 10.0, F.to_timestamp(F.lit(\"2025-08-08 08:15:00\"))),\n",
    "    (1003, \"R003\", \"D012\", \"Pune\", 3.5, 90.0, 0.0, F.to_timestamp(F.lit(\"2025-08-08 08:20:00\"))),\n",
    "    (1004, \"R001\", \"D010\", \"Bengaluru\", 18.9, 480.0, 25.0, F.to_timestamp(F.lit(\"2025-08-08 08:45:00\"))),\n",
    "    (1005, \"R004\", \"D013\", \"Chennai\", 10.0, 260.0, 15.0, F.to_timestamp(F.lit(\"2025-08-08 09:05:00\"))),\n",
    "    (1006, \"R005\", \"D012\", \"Pune\", 2.2, 70.0, 0.0, F.to_timestamp(F.lit(\"2025-08-08 09:10:00\"))),\n",
    "]\n",
    "drivers_rows = [\n",
    "    (\"D010\", \"Anil\", 4.8, \"WagonR\"),\n",
    "    (\"D011\", \"Sana\", 4.6, \"i20\"),\n",
    "    (\"D012\", \"Rakesh\", 4.4, \"Swift\"),\n",
    "    (\"D013\", \"Meera\", 4.9, \"Ciaz\")\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "trips_df = spark.createDataFrame(trips_rows, schema=trip_schema)\n",
    "drivers_df = spark.createDataFrame(drivers_rows, schema=driver_schema)\n",
    "\n",
    "BASE = \"/tmp/delta/ride_hailing\"\n",
    "TRIPS_PATH = f\"{BASE}/trips\"\n",
    "DRIVERS_PATH = f\"{BASE}/drivers\"\n",
    "trips_df.write.format(\"delta\").mode(\"overwrite\").save(TRIPS_PATH)\n",
    "drivers_df.write.format(\"delta\").mode(\"overwrite\").save(DRIVERS_PATH)\n",
    "\n",
    "print(\"Seeded:\")\n",
    "print(\" Trips ->\", TRIPS_PATH)\n",
    "print(\" Drivers ->\", DRIVERS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9656380-389b-432d-badf-325bb50190b6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "1) Managed vs Unmanaged Table"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Managed table (trips_managed)\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE trips_managed USING DELTA LOCATION '{TRIPS_PATH}'\n",
    "AS SELECT * FROM delta.`{TRIPS_PATH}`\n",
    "\"\"\")\n",
    "\n",
    "# Unmanaged table (drivers_ext)\n",
    "spark.sql(f\"\"\"\n",
    "CREATE EXTERNAL TABLE drivers_ext USING DELTA LOCATION '{DRIVERS_PATH}'\n",
    "\"\"\")\n",
    "\n",
    "# Verify with DESCRIBE DETAIL\n",
    "spark.sql(\"DESCRIBE DETAIL trips_managed\").show(truncate=False)\n",
    "spark.sql(\"DESCRIBE DETAIL drivers_ext\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4712b2f7-e68b-4b90-87f9-368b8f5c4c21",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "2) Read & Explore"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "trips_df = spark.read.format(\"delta\").load(TRIPS_PATH)\n",
    "drivers_df = spark.read.format(\"delta\").load(DRIVERS_PATH)\n",
    "\n",
    "# Show schemas and top 10 rows\n",
    "trips_df.printSchema()\n",
    "drivers_df.printSchema()\n",
    "trips_df.show(10)\n",
    "drivers_df.show(10)\n",
    "\n",
    "# Compute derived column 'total_amount' and show top 5 trips by total_amount\n",
    "trips_df = trips_df.withColumn(\"total_amount\", F.col(\"fare\") + F.col(\"tip\"))\n",
    "trips_df.orderBy(F.desc(\"total_amount\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f606d10-83e2-4aa9-8bae-68362d185cf8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "3) Update (Business Rule)"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Increase tip by 5 for trips in Bengaluru where distance_km > 15\n",
    "trips_before_update = trips_df.filter((F.col(\"city\") == \"Bengaluru\") & (F.col(\"distance_km\") > 15))\n",
    "trips_before_update.show()\n",
    "\n",
    "# Perform the update\n",
    "trips_df = trips_df.withColumn(\"tip\", F.when((F.col(\"city\") == \"Bengaluru\") & (F.col(\"distance_km\") > 15), F.col(\"tip\") + 5).otherwise(F.col(\"tip\")))\n",
    "\n",
    "# Show updated rows\n",
    "trips_after_update = trips_df.filter((F.col(\"city\") == \"Bengaluru\") & (F.col(\"distance_km\") > 15))\n",
    "trips_after_update.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7d893aa-7912-41cd-9f17-c5d906e33a25",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "4) Delete (Data Quality)"
    }
   },
   "outputs": [],
   "source": [
    "trips_df = trips_df.filter((F.col(\"fare\") > 0) & (F.col(\"distance_km\") > 0))\n",
    "\n",
    "trips_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "028f3200-cf3a-47ce-a609-e26883534280",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "5) Merge (Upsert New Batch)"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "new_trip_rows = [\n",
    "    (1004, \"R001\", \"D010\", \"Bengaluru\", 18.9, 500.0, 30.0, F.to_timestamp(F.lit(\"2025-08-08 08:45:00\"))),\n",
    "    (1007, \"R006\", \"D013\", \"Mumbai\", 15.0, 350.0, 20.0, F.to_timestamp(F.lit(\"2025-08-08 10:00:00\")))\n",
    "]\n",
    "new_trip_df = spark.createDataFrame(new_trip_rows, schema=trip_schema)\n",
    "\n",
    "# Merge into the existing trips table\n",
    "new_trip_df.createOrReplaceTempView(\"new_trips\")\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO delta.`/tmp/delta/ride_hailing/trips` AS trips\n",
    "USING new_trips AS new\n",
    "ON trips.trip_id = new.trip_id\n",
    "WHEN MATCHED THEN UPDATE SET trips.fare = new.fare, trips.tip = new.tip\n",
    "WHEN NOT MATCHED THEN INSERT VALUES (new.trip_id, new.rider_id, new.driver_id, new.city, new.distance_km, new.fare, new.tip, new.ts)\n",
    "\"\"\")\n",
    "\n",
    "# Verify the merge results\n",
    "spark.sql(\"SELECT * FROM delta.`/tmp/delta/ride_hailing/trips` WHERE trip_id IN (1004, 1007)\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5ad1880-8d54-4aaf-b554-1f5cc9470921",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "6) Gold View (Join & KPIs"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Join trips with drivers\n",
    "gold_view = trips_df.join(drivers_df, \"driver_id\").select(\n",
    "    \"trip_id\", \"city\", \"driver_name\", \"rating\", \"distance_km\", (F.col(\"fare\") + F.col(\"tip\")).alias(\"total_amount\"), \"ts\"\n",
    ")\n",
    "\n",
    "# Compute city-wise total revenue and average driver rating\n",
    "gold_view.groupBy(\"city\").agg(\n",
    "    F.sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "    F.avg(\"rating\").alias(\"avg_driver_rating\")\n",
    ").show()\n",
    "\n",
    "# Compute driver-wise total trips and top 3 drivers by revenue\n",
    "gold_view.groupBy(\"driver_name\").agg(\n",
    "    F.count(\"trip_id\").alias(\"total_trips\"),\n",
    "    F.sum(\"total_amount\").alias(\"total_revenue\")\n",
    ").orderBy(F.desc(\"total_revenue\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4b27a7b-7752-4a64-a270-c226c0337036",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "7) Time Travel & History"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Show DESCRIBE HISTORY\n",
    "spark.sql(\"DESCRIBE HISTORY delta.`/tmp/delta/ride_hailing/trips`\").show()\n",
    "\n",
    "# Read the table as of version 0 and compare with the latest version\n",
    "trips_version_0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(TRIPS_PATH)\n",
    "trips_df = spark.read.format(\"delta\").load(TRIPS_PATH)\n",
    "\n",
    "# Compare row counts and show some rows\n",
    "print(\"Version 0 row count:\", trips_version_0.count())\n",
    "print(\"Latest version row count:\", trips_df.count())\n",
    "trips_version_0.show(5)\n",
    "trips_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3749b2b-0ed7-4cd7-80c2-96c5a3f2f72e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "8) Partitioned Rewrite (Performance)"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "PARTITIONED_PATH = \"/tmp/delta/partitioned_trips\"\n",
    "trips_df.write.partitionBy(\"city\").format(\"delta\").mode(\"overwrite\").save(PARTITIONED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1060cb8e-9958-4e3b-ad1a-a7f75ac9a3f8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "9) Incremental Load Simulation"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Create a small incremental batch\n",
    "incremental_batch = [\n",
    "    (1008, \"R007\", \"D011\", \"Mumbai\", 8.0, 200.0, 12.0, F.to_timestamp(F.lit(\"2025-08-08 11:00:00\"))),\n",
    "    (1009, \"R008\", \"D010\", \"Mumbai\", 10.5, 250.0, 15.0, F.to_timestamp(F.lit(\"2025-08-08 11:05:00\")))\n",
    "]\n",
    "incremental_df = spark.createDataFrame(incremental_batch, schema=trip_schema)\n",
    "\n",
    "# Append to the trips table\n",
    "incremental_df.write.format(\"delta\").mode(\"append\").save(TRIPS_PATH)\n",
    "\n",
    "# Re-run the city-wise revenue aggregation\n",
    "gold_view = spark.read.format(\"delta\").load(TRIPS_PATH).join(drivers_df, \"driver_id\").select(\n",
    "    \"trip_id\", \"city\", \"driver_name\", \"rating\", \"distance_km\", (F.col(\"fare\") + F.col(\"tip\")).alias(\"total_amount\"), \"ts\"\n",
    ")\n",
    "gold_view.groupBy(\"city\").agg(\n",
    "    F.sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "    F.avg(\"rating\").alias(\"avg_driver_rating\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc65608f-4135-45e6-a1d3-6fce71729605",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "10) Simple Streaming (File Stream → Console)"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "streaming_df = spark.readStream.format(\"delta\").load(TRIPS_PATH)\n",
    "query = streaming_df.writeStream.outputMode(\"append\").format(\"console\").trigger(processingTime=\"5 seconds\").start()\n",
    "\n",
    "# Simulate appending a new batch while the stream runs\n",
    "streaming_batch = [\n",
    "    (1010, \"R009\", \"D011\", \"Mumbai\", 7.5, 180.0, 10.0, F.to_timestamp(F.lit(\"2025-08-08 12:00:00\")))\n",
    "]\n",
    "streaming_batch_df = spark.createDataFrame(streaming_batch, schema=trip_schema)\n",
    "streaming_batch_df.write.format(\"delta\").mode(\"append\").save(TRIPS_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4628678e-fff4-4558-a9d8-488dd1bcb663",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "11) Visualizatio"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "city_revenue_df = gold_view.groupBy(\"city\").agg(\n",
    "    F.sum(\"total_amount\").alias(\"total_revenue\")\n",
    ").toPandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "city_revenue_df.plot(kind=\"bar\", x=\"city\", y=\"total_revenue\")\n",
    "plt.title(\"City-wise Revenue\")\n",
    "plt.ylabel(\"Total Revenue\")\n",
    "plt.show()\n",
    "hourly_revenue_df = gold_view.withColumn(\"hour\", F.hour(\"ts\")).groupBy(\"hour\").agg(\n",
    "    F.sum(\"total_amount\").alias(\"hourly_revenue\")\n",
    ").toPandas()\n",
    "plt.plot(hourly_revenue_df[\"hour\"], hourly_revenue_df[\"hourly_revenue\"])\n",
    "plt.title(\"Revenue by Hour\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Hourly Revenue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8109e7a8-57a4-4c4d-a7d4-37b1a0bb53ec",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "12) Managed vs Unmanaged Cleanup"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS trips_managed\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS drivers_ext\")\n",
    "from os import path\n",
    "print(\"Is managed trips data removed?\", not path.exists(TRIPS_PATH))\n",
    "print(\"Is unmanaged drivers data still there?\", path.exists(DRIVERS_PATH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84ce2be6-c108-4c94-92cc-879ba8457d63",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "13) Constraint/Quality Check"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "bad_trip_rows = [\n",
    "    (1011, \"R010\", \"D014\", \"Mumbai\", 10.0, 200.0, -5.0, F.to_timestamp(F.lit(\"2025-08-08 12:10:00\"))),  # Negative tip\n",
    "    (1012, \"R011\", \"D015\", \"Chennai\", 7.5, 150.0, 0.0, F.to_timestamp(F.lit(\"2025-08-08 12:20:00\")))\n",
    "]\n",
    "bad_trip_df = spark.createDataFrame(bad_trip_rows, schema=trip_schema)\n",
    "valid_trip_df = bad_trip_df.filter(F.col(\"tip\") >= 0)\n",
    "valid_trip_df.write.format(\"delta\").mode(\"append\").save(TRIPS_PATH)\n",
    "\n",
    "try:\n",
    "    bad_trip_df.write.format(\"delta\").mode(\"append\").save(TRIPS_PATH)\n",
    "except Exception as e:\n",
    "    print(\"Error writing bad data:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3572bf4d-20f5-4b8d-90f7-1e25384b860b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "14) Convert Parquet"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "parquet_path = \"/tmp/delta/ride_hailing_parquet\"\n",
    "subset_trips_df = trips_df.limit(5)  # Take a small subset of trips\n",
    "subset_trips_df.write.format(\"parquet\").mode(\"overwrite\").save(parquet_path)\n",
    "\n",
    "delta_path = \"/tmp/delta/ride_hailing_delta_from_parquet\"\n",
    "spark.read.parquet(parquet_path).write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "delta_df = spark.read.format(\"delta\").load(delta_path)\n",
    "delta_df_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_path)\n",
    "\n",
    "delta_df_v0.show(5)\n",
    "delta_df.show(5)\n",
    "\n",
    "new_batch_df = spark.createDataFrame([(1006, \"R009\", \"D013\", \"Mumbai\", 8.0, 150.0, 10.0, F.to_timestamp(F.lit(\"2025-08-08 12:25:00\")))],\n",
    "schema=trip_schema)\n",
    "\n",
    "new_batch_df.createOrReplaceTempView(\"new_batch\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO delta.`/tmp/delta/ride_hailing_delta_from_parquet` AS trips\n",
    "USING new_batch AS new\n",
    "ON trips.trip_id = new.trip_id\n",
    "WHEN MATCHED THEN UPDATE SET trips.fare = new.fare, trips.tip = new.tip\n",
    "WHEN NOT MATCHED THEN INSERT VALUES (new.trip_id, new.rider_id, new.driver_id, new.city, new.distance_km, new.fare, new.tip, new.ts)\n",
    "\"\"\")\n",
    "\n",
    "spark.read.format(\"delta\").load(delta_path).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2581fcdd-71df-4c67-8807-fa7d2d29442e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "15) Bonus KPI Dashboard"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert to Pandas for plotting\n",
    "city_revenue_df = gold_view.groupBy(\"city\").agg(\n",
    "    F.sum(\"total_amount\").alias(\"total_revenue\")\n",
    ").toPandas()\n",
    "\n",
    "# Plotting (Trips per City - Bar Chart)\n",
    "city_revenue_df.plot(kind=\"bar\", x=\"city\", y=\"total_revenue\")\n",
    "plt.title(\"City-wise Revenue\")\n",
    "plt.ylabel(\"Total Revenue\")\n",
    "plt.xlabel(\"City\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting (Top Drivers by Revenue - Bar Chart)\n",
    "driver_revenue_df = gold_view.groupBy(\"driver_name\").agg(\n",
    "    F.sum(\"total_amount\").alias(\"total_revenue\")\n",
    ").orderBy(F.desc(\"total_revenue\")).limit(10).toPandas()\n",
    "\n",
    "driver_revenue_df.plot(kind=\"bar\", x=\"driver_name\", y=\"total_revenue\")\n",
    "plt.title(\"Top 10 Drivers by Revenue\")\n",
    "plt.ylabel(\"Total Revenue\")\n",
    "plt.xlabel(\"Driver Name\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting (Revenue by Hour - Line Chart)\n",
    "hourly_revenue_df = gold_view.withColumn(\"hour\", F.hour(\"ts\")).groupBy(\"hour\").agg(\n",
    "    F.sum(\"total_amount\").alias(\"hourly_revenue\")\n",
    ").toPandas()\n",
    "\n",
    "plt.plot(hourly_revenue_df[\"hour\"], hourly_revenue_df[\"hourly_revenue\"])\n",
    "plt.title(\"Revenue by Hour\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Hourly Revenue\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4481400-3366-40cf-b4d3-abe4726316eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51a6c4d9-9275-4a9f-b0eb-8c302bcb8aeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Azure Databricks Assessment 2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}