{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Databricks Notebook: Smart Home Energy ETL\n",
        "# Week 4 - Databricks ETL for Smart Energy Monitoring\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Step 1: Setup widgets (so you can change file paths easily)\n",
        "# -------------------------------------------------------"
      ],
      "metadata": {
        "id": "JZrh74ScrZ9U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTXRJ7lmqWRg"
      },
      "outputs": [],
      "source": [
        "\n",
        "dbutils.widgets.text(\"input_path\", \"/FileStore/tables/energyusage.csv\")\n",
        "dbutils.widgets.text(\"delta_db\", \"smart_energy_db\")\n",
        "dbutils.widgets.text(\"out_dir\", \"/FileStore/tables/smart_energy_outputs\")\n",
        "dbutils.widgets.text(\"alert_threshold_kwh\", \"10\")\n",
        "\n",
        "INPUT_PATH = dbutils.widgets.get(\"input_path\")\n",
        "DELTA_DB   = dbutils.widgets.get(\"delta_db\")\n",
        "OUT_DIR    = dbutils.widgets.get(\"out_dir\")\n",
        "ALERT_KWH  = float(dbutils.widgets.get(\"alert_threshold_kwh\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DELTA_DB}\")\n",
        "dbutils.fs.mkdirs(OUT_DIR)"
      ],
      "metadata": {
        "id": "9Ca8CVIOrhld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 3: Read cleaned logs from CSV\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, TimestampType\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"device_id\", IntegerType(), True),\n",
        "    StructField(\"room_id\", IntegerType(), True),\n",
        "    StructField(\"timestamp\", TimestampType(), True),\n",
        "    StructField(\"energy_kwh\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "# Load CSV\n",
        "df = (spark.read\n",
        "      .option(\"header\", True)\n",
        "      .schema(schema)\n",
        "      .csv(INPUT_PATH))\n",
        "\n",
        "# Clean: drop nulls and invalid readings\n",
        "df_clean = (df\n",
        "    .dropna(subset=[\"device_id\",\"room_id\",\"timestamp\",\"energy_kwh\"])\n",
        "    .filter(col(\"energy_kwh\") > 0)\n",
        ")\n"
      ],
      "metadata": {
        "id": "FqNj_tXSrhn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 4: Add day/week columns for aggregation\n",
        "\n",
        "from pyspark.sql.functions import to_date, date_trunc\n",
        "\n",
        "df_enriched = (df_clean\n",
        "    .withColumn(\"day\", to_date(col(\"timestamp\")))\n",
        "    .withColumn(\"week\", date_trunc(\"week\", col(\"timestamp\")))\n",
        ")\n"
      ],
      "metadata": {
        "id": "_zMR2fenrhpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Daily summary (sum, avg, count)\n",
        "\n",
        "from pyspark.sql.functions import sum as s_sum, avg as s_avg, count as s_count\n",
        "\n",
        "daily = (df_enriched\n",
        "    .groupBy(\"day\",\"device_id\",\"room_id\")\n",
        "    .agg(\n",
        "        s_sum(\"energy_kwh\").alias(\"daily_kwh\"),\n",
        "        s_avg(\"energy_kwh\").alias(\"daily_avg_kwh\"),\n",
        "        s_count(\"*\").alias(\"readings\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# Save results in Delta + CSV\n",
        "daily.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{DELTA_DB}.daily_usage\")\n",
        "daily.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(f\"{OUT_DIR}/daily_usage_csv\")"
      ],
      "metadata": {
        "id": "Ejy04rMjrhsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 6: Weekly summary (sum, avg, count)\n",
        "\n",
        "weekly = (df_enriched\n",
        "    .groupBy(\"week\",\"device_id\",\"room_id\")\n",
        "    .agg(\n",
        "        s_sum(\"energy_kwh\").alias(\"weekly_kwh\"),\n",
        "        s_avg(\"energy_kwh\").alias(\"weekly_avg_kwh\"),\n",
        "        s_count(\"*\").alias(\"readings\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# Save results in Delta + CSV\n",
        "weekly.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{DELTA_DB}.weekly_usage\")\n",
        "weekly.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(f\"{OUT_DIR}/weekly_usage_csv\")"
      ],
      "metadata": {
        "id": "qBPa-MUUrhvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 7: Peak vs Off-Peak usage classification\n",
        "\n",
        "from pyspark.sql.functions import hour, when\n",
        "\n",
        "classified = (df_enriched\n",
        "    .withColumn(\"hour\", hour(col(\"timestamp\")))\n",
        "    .withColumn(\"usage_type\", when((col(\"hour\") >= 18) & (col(\"hour\") <= 23), \"peak\").otherwise(\"off_peak\"))\n",
        ")\n",
        "\n",
        "# Pivot to show peak/off-peak usage side by side\n",
        "pv = (classified\n",
        "    .groupBy(\"device_id\",\"usage_type\")\n",
        "    .agg(s_sum(\"energy_kwh\").alias(\"total_kwh\"))\n",
        "    .groupBy(\"device_id\")\n",
        "    .pivot(\"usage_type\", [\"peak\",\"off_peak\"]).sum(\"total_kwh\")\n",
        "    .na.fill(0.0)\n",
        "    .withColumn(\"total_usage\", col(\"peak\")+col(\"off_peak\"))\n",
        ")\n",
        "\n",
        "# Save results\n",
        "pv.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{DELTA_DB}.peak_offpeak_usage\")\n",
        "pv.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(f\"{OUT_DIR}/peak_offpeak_usage_csv\")"
      ],
      "metadata": {
        "id": "qQQDsAxyrhy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 8: Alerts for devices exceeding daily threshold\n",
        "\n",
        "alerts = spark.sql(f'''\n",
        "  SELECT day, device_id, room_id, daily_kwh\n",
        "  FROM {DELTA_DB}.daily_usage\n",
        "  WHERE daily_kwh > {ALERT_KWH}\n",
        "  ORDER BY daily_kwh DESC\n",
        "''')\n",
        "\n",
        "# Save alerts to CSV for dashboard/notifications\n",
        "alerts.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(f\"{OUT_DIR}/alerts_csv\")\n"
      ],
      "metadata": {
        "id": "uv_eSfjjrh1W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}