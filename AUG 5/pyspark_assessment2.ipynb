
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIjpdgnwGVDA"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BotCampus PySpark Practice\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"Anjali\", \"Bangalore\", 24),\n",
        "    (\"Ravi\", \"Hyderabad\", 28),\n",
        "    (\"Kavya\", \"Delhi\", 22),\n",
        "    (\"Meena\", \"Chennai\", 25),\n",
        "    (\"Arjun\", \"Mumbai\", 30)\n",
        "]\n",
        "\n",
        "columns = [\"name\", \"city\", \"age\"]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "df.show()\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMKbV-pGJB6J",
        "outputId": "6d74865e-3749-47d5-ecba-9a8dc82dea38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+---+\n",
            "|  name|     city|age|\n",
            "+------+---------+---+\n",
            "|Anjali|Bangalore| 24|\n",
            "|  Ravi|Hyderabad| 28|\n",
            "| Kavya|    Delhi| 22|\n",
            "| Meena|  Chennai| 25|\n",
            "| Arjun|   Mumbai| 30|\n",
            "+------+---------+---+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = df.rdd\n",
        "print(type(rdd))\n",
        "print(df.collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BQ2FZ7OJB9B",
        "outputId": "683b949b-11dc-4bb1-86e2-0e0699e375a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n",
            "[Row(name='Anjali', city='Bangalore', age=24), Row(name='Ravi', city='Hyderabad', age=28), Row(name='Kavya', city='Delhi', age=22), Row(name='Meena', city='Chennai', age=25), Row(name='Arjun', city='Mumbai', age=30)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mapped_rdd = df.rdd.map(lambda row: f\"{row.name} lives in {row.city} and is {row.age} years old\")\n",
        "for line in mapped_rdd.collect():\n",
        "    print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99Ajs14NJB_7",
        "outputId": "079d7520-99e3-489c-b6d5-1a0f67e61b63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anjali lives in Bangalore and is 24 years old\n",
            "Ravi lives in Hyderabad and is 28 years old\n",
            "Kavya lives in Delhi and is 22 years old\n",
            "Meena lives in Chennai and is 25 years old\n",
            "Arjun lives in Mumbai and is 30 years old\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Module 2: RDDs & Transformations\n",
        "\n",
        "Scenario: You received app feedback from users in free-text.\n",
        "\n",
        "feedback = spark.sparkContext.parallelize([\n",
        "\n",
        "\"Ravi from Bangalore loved the delivery\",\n",
        "\n",
        "\"Meena from Hyderabad had a late order\",\n",
        "\n",
        "\"Ajay from Pune liked the service\",\n",
        "\n",
        "\"Anjali from Delhi faced UI issues\",\n",
        "\n",
        "\"Rohit from Mumbai gave positive feedback\"\n",
        "])\n",
        "\n",
        "Tasks:\n",
        "\n",
        "Split each line into words ( flatMap ).\n",
        "\n",
        "Remove stop words ( from , the , etc.).\n",
        "\n",
        "Count each word frequency using reduceByKey .\n",
        "\n",
        "Find top 3 most frequent non-stop words."
      ],
      "metadata": {
        "id": "xiVfSNybJdCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feedback = spark.sparkContext.parallelize([\n",
        "    \"Ravi from Bangalore loved the delivery\",\n",
        "    \"Meena from Hyderabad had a late order\",\n",
        "    \"Ajay from Pune liked the service\",\n",
        "    \"Anjali from Delhi faced UI issues\",\n",
        "    \"Rohit from Mumbai gave positive feedback\"\n",
        "])\n"
      ],
      "metadata": {
        "id": "hV__WDyqJCCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Tokenize\n",
        "words = feedback.flatMap(lambda line: line.lower().split())\n",
        "words.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBIc0HzaJCFN",
        "outputId": "856dae79-c46b-4a7e-b2e1-78d5f9645a48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ravi',\n",
              " 'from',\n",
              " 'bangalore',\n",
              " 'loved',\n",
              " 'the',\n",
              " 'delivery',\n",
              " 'meena',\n",
              " 'from',\n",
              " 'hyderabad',\n",
              " 'had',\n",
              " 'a',\n",
              " 'late',\n",
              " 'order',\n",
              " 'ajay',\n",
              " 'from',\n",
              " 'pune',\n",
              " 'liked',\n",
              " 'the',\n",
              " 'service',\n",
              " 'anjali',\n",
              " 'from',\n",
              " 'delhi',\n",
              " 'faced',\n",
              " 'ui',\n",
              " 'issues',\n",
              " 'rohit',\n",
              " 'from',\n",
              " 'mumbai',\n",
              " 'gave',\n",
              " 'positive',\n",
              " 'feedback']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 2: Remove stop words\n",
        "stop_words = {'from', 'the', 'a', 'had', 'and', 'is', 'in', 'of', 'to'}\n",
        "filtered_words = words.filter(lambda word: word not in stop_words)\n",
        "filtered_words.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJFFDluNJCL7",
        "outputId": "93f34779-ef73-4934-a609-f072dfaf1cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ravi',\n",
              " 'bangalore',\n",
              " 'loved',\n",
              " 'delivery',\n",
              " 'meena',\n",
              " 'hyderabad',\n",
              " 'late',\n",
              " 'order',\n",
              " 'ajay',\n",
              " 'pune',\n",
              " 'liked',\n",
              " 'service',\n",
              " 'anjali',\n",
              " 'delhi',\n",
              " 'faced',\n",
              " 'ui',\n",
              " 'issues',\n",
              " 'rohit',\n",
              " 'mumbai',\n",
              " 'gave',\n",
              " 'positive',\n",
              " 'feedback']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 3: Word count\n",
        "word_counts = filtered_words.map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)\n",
        "word_counts.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CinGEc0J03x",
        "outputId": "78cfaea8-65cb-40a1-9f19-bb8051abcec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('loved', 1),\n",
              " ('liked', 1),\n",
              " ('service', 1),\n",
              " ('anjali', 1),\n",
              " ('faced', 1),\n",
              " ('issues', 1),\n",
              " ('rohit', 1),\n",
              " ('mumbai', 1),\n",
              " ('positive', 1),\n",
              " ('feedback', 1),\n",
              " ('ravi', 1),\n",
              " ('bangalore', 1),\n",
              " ('delivery', 1),\n",
              " ('meena', 1),\n",
              " ('hyderabad', 1),\n",
              " ('late', 1),\n",
              " ('order', 1),\n",
              " ('ajay', 1),\n",
              " ('pune', 1),\n",
              " ('delhi', 1),\n",
              " ('ui', 1),\n",
              " ('gave', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 4: Top 3 most frequent words\n",
        "top_3_words = word_counts.takeOrdered(3, key=lambda x: -x[1])\n",
        "\n",
        "# Print result\n",
        "for word, count in top_3_words:\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7O4x_2lJ06r",
        "outputId": "e49a8f09-b07a-46bd-bf9c-af6e77cb6c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loved: 1\n",
            "liked: 1\n",
            "service: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tasks:\n",
        "Join both DataFrames on name .\n",
        "\n",
        "Create a new column: attendance_rate = days_present / 25 .\n",
        "\n",
        "Grade students using when :\n",
        "\n",
        "A: >90, B: 80–90, C: <80.\n",
        "\n",
        "Filter students with good grades but poor attendance (<80%)."
      ],
      "metadata": {
        "id": "DPSoBh7fkMWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Students Data\n",
        "students = [\n",
        "    (\"Amit\", \"10-A\", 89),\n",
        "    (\"Kavya\", \"10-B\", 92),\n",
        "    (\"Anjali\", \"10-A\", 78),\n",
        "    (\"Rohit\", \"10-B\", 85),\n",
        "    (\"Sneha\", \"10-C\", 80)\n",
        "]\n",
        "columns = [\"name\", \"section\", \"marks\"]\n",
        "\n",
        "# Attendance Data\n",
        "attendance = [\n",
        "    (\"Amit\", 24),\n",
        "    (\"Kavya\", 22),\n",
        "    (\"Anjali\", 20),\n",
        "    (\"Rohit\", 25),\n",
        "    (\"Sneha\", 19)\n",
        "]\n",
        "columns2 = [\"name\", \"days_present\"]\n"
      ],
      "metadata": {
        "id": "zkEC_zwgkRzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "students_df = spark.createDataFrame(students, columns)\n",
        "attendance_df = spark.createDataFrame(attendance, columns2)\n"
      ],
      "metadata": {
        "id": "V-uLxaYBkR3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df = students_df.join(attendance_df, on=\"name\")\n",
        "joined_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbWxoY5XkR7o",
        "outputId": "a86af806-e0c4-4c30-b84f-32fb8d36a83e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+------------+\n",
            "|  name|section|marks|days_present|\n",
            "+------+-------+-----+------------+\n",
            "|  Amit|   10-A|   89|          24|\n",
            "|Anjali|   10-A|   78|          20|\n",
            "| Kavya|   10-B|   92|          22|\n",
            "| Rohit|   10-B|   85|          25|\n",
            "| Sneha|   10-C|   80|          19|\n",
            "+------+-------+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "joined_df = joined_df.withColumn(\"attendance_rate\", col(\"days_present\") / 25)\n",
        "joined_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2kl6-rqkR-K",
        "outputId": "85295d56-8070-45e4-ba2b-6fcd261a2bdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+------------+---------------+\n",
            "|  name|section|marks|days_present|attendance_rate|\n",
            "+------+-------+-----+------------+---------------+\n",
            "|  Amit|   10-A|   89|          24|           0.96|\n",
            "|Anjali|   10-A|   78|          20|            0.8|\n",
            "| Kavya|   10-B|   92|          22|           0.88|\n",
            "| Rohit|   10-B|   85|          25|            1.0|\n",
            "| Sneha|   10-C|   80|          19|           0.76|\n",
            "+------+-------+-----+------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import round, when, col\n",
        "\n",
        "graded_df = joined_df.withColumn(\"grade\", when(col(\"marks\") > 90, \"A\")\n",
        "                                 .when((col(\"marks\") <= 90) & (col(\"marks\") >= 80), \"B\")\n",
        "                                 .otherwise(\"C\"))\n",
        "\n",
        "graded_df = graded_df.withColumn(\"attendance_rate\", round(col(\"attendance_rate\") * 100, 2))  # in %\n",
        "graded_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjDs8IRJkSBr",
        "outputId": "581fdf16-dcaa-4fb4-c7b4-bec83a010f55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+------------+---------------+-----+\n",
            "|  name|section|marks|days_present|attendance_rate|grade|\n",
            "+------+-------+-----+------------+---------------+-----+\n",
            "|  Amit|   10-A|   89|          24|           96.0|    B|\n",
            "|Anjali|   10-A|   78|          20|           80.0|    C|\n",
            "| Kavya|   10-B|   92|          22|           88.0|    A|\n",
            "| Rohit|   10-B|   85|          25|          100.0|    B|\n",
            "| Sneha|   10-C|   80|          19|           76.0|    B|\n",
            "+------+-------+-----+------------+---------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df = graded_df.filter(\n",
        "    (col(\"grade\").isin(\"A\", \"B\")) & (col(\"attendance_rate\") < 80)\n",
        ")\n",
        "filtered_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZxrfPvXkSGm",
        "outputId": "e6683db3-e3f3-49e0-e882-f9ef5959f91e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+-----+------------+---------------+-----+\n",
            "| name|section|marks|days_present|attendance_rate|grade|\n",
            "+-----+-------+-----+------------+---------------+-----+\n",
            "|Sneha|   10-C|   80|          19|           76.0|    B|\n",
            "+-----+-------+-----+------------+---------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Module 4: Ingest CSV & JSON, Save to Parquet\n",
        "Tasks:\n",
        "1. Ingest CSV:\n",
        "emp_id,name,dept,city,salary\n",
        "101,Anil,IT,Bangalore,80000\n",
        "102,Kiran,HR,Mumbai,65000\n",
        "103,Deepa,Finance,Chennai,72000\n",
        "2. Ingest JSON:\n",
        "{\n",
        "\"id\": 201,\n",
        "\"name\": \"Nandini\",\n",
        "\"contact\": {\n",
        "\"email\": \"nandi@example.com\",\n",
        "\"city\": \"Hyderabad\"\n",
        "},\n",
        "\"skills\": [\"Python\", \"Spark\", \"SQL\"]\n",
        "}\n",
        "Tasks:\n",
        "Read both formats into DataFrames.\n",
        "\n",
        "Flatten nested JSON using select , col , alias , explode .\n",
        "Save both as Parquet files partitioned by city."
      ],
      "metadata": {
        "id": "GsiYMElilfUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_data = \"\"\"emp_id,name,dept,city,salary\n",
        "101,Anil,IT,Bangalore,80000\n",
        "102,Kiran,HR,Mumbai,65000\n",
        "103,Deepa,Finance,Chennai,72000\n",
        "\"\"\"\n",
        "\n",
        "with open(\"employees.csv\", \"w\") as f:\n",
        "    f.write(csv_data)\n"
      ],
      "metadata": {
        "id": "10Grho0skSJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_data = '''\n",
        "{\n",
        "  \"id\": 201,\n",
        "  \"name\": \"Nandini\",\n",
        "  \"contact\": {\n",
        "    \"email\": \"nandi@example.com\",\n",
        "    \"city\": \"Hyderabad\"\n",
        "  },\n",
        "  \"skills\": [\"Python\", \"Spark\", \"SQL\"]\n",
        "}\n",
        "'''\n",
        "\n",
        "with open(\"employee.json\", \"w\") as f:\n",
        "    f.write(json_data)\n"
      ],
      "metadata": {
        "id": "DwomzQwnlrPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emp_df = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)\n",
        "emp_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4W5o9Q3lrTb",
        "outputId": "8e0751c3-9b3d-4854-f983-12b24de72c30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+---------+------+\n",
            "|emp_id| name|   dept|     city|salary|\n",
            "+------+-----+-------+---------+------+\n",
            "|   101| Anil|     IT|Bangalore| 80000|\n",
            "|   102|Kiran|     HR|   Mumbai| 65000|\n",
            "|   103|Deepa|Finance|  Chennai| 72000|\n",
            "+------+-----+-------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_df = spark.read.json(\"employee.json\", multiLine=True)\n",
        "json_df.printSchema()\n",
        "json_df.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Y8a6RKmlrYF",
        "outputId": "e8a41e02-9795-4c9a-9db5-311823964aec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- contact: struct (nullable = true)\n",
            " |    |-- city: string (nullable = true)\n",
            " |    |-- email: string (nullable = true)\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- skills: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+------------------------------+---+-------+--------------------+\n",
            "|contact                       |id |name   |skills              |\n",
            "+------------------------------+---+-------+--------------------+\n",
            "|{Hyderabad, nandi@example.com}|201|Nandini|[Python, Spark, SQL]|\n",
            "+------------------------------+---+-------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, explode\n",
        "flat_json_df = json_df.select(\n",
        "    col(\"id\"),\n",
        "    col(\"name\"),\n",
        "    col(\"contact.email\").alias(\"email\"),\n",
        "    col(\"contact.city\").alias(\"city\"),\n",
        "    explode(col(\"skills\")).alias(\"skill\")\n",
        ")\n",
        "flat_json_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-CFNjw4lrbG",
        "outputId": "ec0b63c9-1043-4ca4-f1cb-293f933c572c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+-----------------+---------+------+\n",
            "| id|   name|            email|     city| skill|\n",
            "+---+-------+-----------------+---------+------+\n",
            "|201|Nandini|nandi@example.com|Hyderabad|Python|\n",
            "|201|Nandini|nandi@example.com|Hyderabad| Spark|\n",
            "|201|Nandini|nandi@example.com|Hyderabad|   SQL|\n",
            "+---+-------+-----------------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp_df.write.mode(\"overwrite\").partitionBy(\"city\").parquet(\"parquet_output/employees\")\n",
        "emp_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEeJxI-Dlrdw",
        "outputId": "1abd4de9-09a8-4435-c0bc-d91e604da925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+---------+------+\n",
            "|emp_id| name|   dept|     city|salary|\n",
            "+------+-----+-------+---------+------+\n",
            "|   101| Anil|     IT|Bangalore| 80000|\n",
            "|   102|Kiran|     HR|   Mumbai| 65000|\n",
            "|   103|Deepa|Finance|  Chennai| 72000|\n",
            "+------+-----+-------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flat_json_df.write.mode(\"overwrite\").partitionBy(\"city\").parquet(\"parquet_output/json_employees\")\n",
        "flat_json_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGM9QhzNlrg0",
        "outputId": "252b34d8-0e9f-4410-fbac-7413e5dab4ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+-----------------+---------+------+\n",
            "| id|   name|            email|     city| skill|\n",
            "+---+-------+-----------------+---------+------+\n",
            "|201|Nandini|nandi@example.com|Hyderabad|Python|\n",
            "|201|Nandini|nandi@example.com|Hyderabad| Spark|\n",
            "|201|Nandini|nandi@example.com|Hyderabad|   SQL|\n",
            "+---+-------+-----------------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Module 5: Spark SQL with Temp Views\n",
        "\n",
        "Tasks:\n",
        "\n",
        "Register the students DataFrame as students_view .\n",
        "\n",
        "Write and run the following queries:\n",
        "\n",
        "-- a) Average marks per section\n",
        "\n",
        "-- b) Top scorer in each section\n",
        "\n",
        "-- c) Count of students in each grade category\n",
        "\n",
        "-- d) Students with marks above class average\n",
        "\n",
        "-- e) Attendance-adjusted performance"
      ],
      "metadata": {
        "id": "jFqiLqgNmZMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, avg, round\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BotCampus PySpark Practice\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "students = [\n",
        "    (\"Amit\", \"10-A\", 89),\n",
        "    (\"Kavya\", \"10-B\", 92),\n",
        "    (\"Anjali\", \"10-A\", 78),\n",
        "    (\"Rohit\", \"10-B\", 85),\n",
        "    (\"Sneha\", \"10-C\", 80)\n",
        "]\n",
        "columns = [\"name\", \"section\", \"marks\"]\n",
        "\n",
        "attendance = [\n",
        "    (\"Amit\", 24),\n",
        "    (\"Kavya\", 22),\n",
        "    (\"Anjali\", 20),\n",
        "    (\"Rohit\", 25),\n",
        "    (\"Sneha\", 19)\n",
        "]\n",
        "columns2 = [\"name\", \"days_present\"]\n",
        "\n",
        "students_df = spark.createDataFrame(students, columns)\n",
        "attendance_df = spark.createDataFrame(attendance, columns2)\n"
      ],
      "metadata": {
        "id": "i37udSorlrkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df = students_df.join(attendance_df, \"name\")\n",
        "joined_df = joined_df.withColumn(\"attendance_rate\", round(col(\"days_present\") / 25 * 100, 2))\n",
        "graded_df = joined_df.withColumn(\"grade\", when(col(\"marks\") > 90, \"A\")\n",
        "                                  .when((col(\"marks\") <= 90) & (col(\"marks\") >= 80), \"B\")\n",
        "                                  .otherwise(\"C\"))\n",
        "graded_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnNvkyD9mntg",
        "outputId": "e123ea3c-e62b-4cc7-c166-4011af4972a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+------------+---------------+-----+\n",
            "|  name|section|marks|days_present|attendance_rate|grade|\n",
            "+------+-------+-----+------------+---------------+-----+\n",
            "|  Amit|   10-A|   89|          24|           96.0|    B|\n",
            "|Anjali|   10-A|   78|          20|           80.0|    C|\n",
            "| Kavya|   10-B|   92|          22|           88.0|    A|\n",
            "| Rohit|   10-B|   85|          25|          100.0|    B|\n",
            "| Sneha|   10-C|   80|          19|           76.0|    B|\n",
            "+------+-------+-----+------------+---------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graded_df.createOrReplaceTempView(\"students_view\")\n"
      ],
      "metadata": {
        "id": "vaHzVkoomnxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- a) Average marks per section\n",
        "\n",
        "-- b) Top scorer in each section\n",
        "\n",
        "-- c) Count of students in each grade category\n",
        "\n",
        "-- d) Students with marks above class average\n",
        "\n",
        "-- e) Attendance-adjusted performance"
      ],
      "metadata": {
        "id": "qgObBoFKm3GY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT section, AVG(marks) AS avg_marks FROM students_view GROUP BY section\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Z2bZJr_mn0T",
        "outputId": "33682c1f-2e96-4a60-bf12-c541b95114e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+\n",
            "|section|avg_marks|\n",
            "+-------+---------+\n",
            "|   10-C|     80.0|\n",
            "|   10-A|     83.5|\n",
            "|   10-B|     88.5|\n",
            "+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT name, section, marks FROM students_view WHERE marks = (SELECT MAX(marks) FROM students_view)\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPknPIrOmn3n",
        "outputId": "39af6dc9-0855-48f8-9b04-375f015dcf4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+-----+\n",
            "| name|section|marks|\n",
            "+-----+-------+-----+\n",
            "|Kavya|   10-B|   92|\n",
            "+-----+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT grade, COUNT(*) AS count FROM students_view GROUP BY grade\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9br6whpzmn6w",
        "outputId": "0cc4df44-6f8d-4ec6-d27a-8dfa2b76a3b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|grade|count|\n",
            "+-----+-----+\n",
            "|    B|    3|\n",
            "|    C|    1|\n",
            "|    A|    1|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT name, section, marks FROM students_view WHERE marks > (SELECT AVG(marks) FROM students_view)\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plXBql_qnuEL",
        "outputId": "1740fccf-1c0f-494e-8970-2b032f1612f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+-----+\n",
            "| name|section|marks|\n",
            "+-----+-------+-----+\n",
            "| Amit|   10-A|   89|\n",
            "|Kavya|   10-B|   92|\n",
            "|Rohit|   10-B|   85|\n",
            "+-----+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT name, section, attendance_rate FROM students_view WHERE attendance_rate > 80\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKsg6aCWmn9j",
        "outputId": "b1229c49-f118-411c-9fdd-6473bbe6a224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+---------------+\n",
            "| name|section|attendance_rate|\n",
            "+-----+-------+---------------+\n",
            "| Amit|   10-A|           96.0|\n",
            "|Kavya|   10-B|           88.0|\n",
            "|Rohit|   10-B|          100.0|\n",
            "+-----+-------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Module 6: Partitioned Data & Incremental Loading\n",
        "\n",
        "Step 1: Full Load\n",
        "\n",
        "students_df.write.partitionBy(\"section\").parquet(\"output/students/\")\n",
        "\n",
        "Step 2: Incremental Load\n",
        "\n",
        "incremental = [(\"Tejas\", \"10-A\", 91)]\n",
        "df_inc = spark.createDataFrame(incremental, [\"name\", \"section\", \"marks\"])\n",
        "df_inc.write.mode(\"append\").partitionBy(\"section\").parquet(\"output/students/\")\n",
        "\n",
        "Tasks:\n",
        "\n",
        "List files in output/students/ using Python.\n",
        "\n",
        "Read only partition 10-A and list students.\n",
        "\n",
        "Compare before/after counts for section 10-A ."
      ],
      "metadata": {
        "id": "zNVmZAlBn8on"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "students = [\n",
        "    (\"Amit\", \"10-A\", 89),\n",
        "    (\"Kavya\", \"10-B\", 92),\n",
        "    (\"Anjali\", \"10-A\", 78),\n",
        "    (\"Rohit\", \"10-B\", 85),\n",
        "    (\"Sneha\", \"10-C\", 80)\n",
        "]\n",
        "columns = [\"name\", \"section\", \"marks\"]\n",
        "\n",
        "students_df = spark.createDataFrame(students, columns)\n",
        "\n",
        "# Write partitioned by section\n",
        "students_df.write.mode(\"overwrite\").partitionBy(\"section\").parquet(\"output/students/\")\n"
      ],
      "metadata": {
        "id": "Vab_bG39moBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "incremental = [(\"Tejas\", \"10-A\", 91)]\n",
        "df_inc = spark.createDataFrame(incremental, [\"name\", \"section\", \"marks\"])\n",
        "\n",
        "df_inc.write.mode(\"append\").partitionBy(\"section\").parquet(\"output/students/\")\n",
        "df_inc.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLwLxBEtoGHy",
        "outputId": "9aedffe6-66d2-47e5-a9f8-b3b43c63082a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+-----+\n",
            "| name|section|marks|\n",
            "+-----+-------+-----+\n",
            "|Tejas|   10-A|   91|\n",
            "+-----+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base_path = \"output/students/\"\n",
        "for root, dirs, files in os.walk(base_path):\n",
        "    for name in files:\n",
        "        print(os.path.join(root, name))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHHVnfmqoGM6",
        "outputId": "c8cd4870-a102-44c4-d718-1e7ba474af66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output/students/._SUCCESS.crc\n",
            "output/students/_SUCCESS\n",
            "output/students/section=10-A/part-00001-ba7ef944-4b5e-4f35-9881-fe71cf470024.c000.snappy.parquet\n",
            "output/students/section=10-A/.part-00001-eea46714-9915-40e3-916a-162b56f20a6a.c000.snappy.parquet.crc\n",
            "output/students/section=10-A/part-00000-ba7ef944-4b5e-4f35-9881-fe71cf470024.c000.snappy.parquet\n",
            "output/students/section=10-A/.part-00001-ba7ef944-4b5e-4f35-9881-fe71cf470024.c000.snappy.parquet.crc\n",
            "output/students/section=10-A/part-00001-eea46714-9915-40e3-916a-162b56f20a6a.c000.snappy.parquet\n",
            "output/students/section=10-A/.part-00000-ba7ef944-4b5e-4f35-9881-fe71cf470024.c000.snappy.parquet.crc\n",
            "output/students/section=10-B/part-00001-ba7ef944-4b5e-4f35-9881-fe71cf470024.c000.snappy.parquet\n",
            "output/students/section=10-B/part-00000-ba7ef944-4b5e-4f35-9881-fe71cf470024.c000.snappy.parquet\n",
            "output/students/section=10-B/.part-00001-ba7ef944-4b5e-4f35-9881-fe71cf470024.c000.snappy.parquet.crc\n",
            "output/students/section=10-B/.part-00000-ba7ef944-4b5e-4f35-9881-fe71cf470024.c000.snappy.parquet.crc\n",
            "output/students/section=10-C/part-00001-ba7ef944-4b5e-4f35-9881-fe71cf470024.c000.snappy.parquet\n",
            "output/students/section=10-C/.part-00001-ba7ef944-4b5e-4f35-9881-fe71cf470024.c000.snappy.parquet.crc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_10a = spark.read.parquet(\"output/students/section=10-A\")\n",
        "df_10a.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i73t731XoGPc",
        "outputId": "1169a940-15ce-4804-f88a-9831e7c88c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|  name|marks|\n",
            "+------+-----+\n",
            "|Anjali|   78|\n",
            "| Tejas|   91|\n",
            "|  Amit|   89|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_students_df = spark.read.parquet(\"output/students/\")\n"
      ],
      "metadata": {
        "id": "1gYJB79YoGSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_10a = all_students_df.filter(col(\"section\") == \"10-A\").count()\n",
        "print(f\"Total students in 10-A after incremental load: {count_10a}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIAnqEKyogNI",
        "outputId": "a365428e-ede4-44c5-8588-2a4a85de8186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total students in 10-A after incremental load: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Module 7: ETL Pipeline – End to End\n",
        "Given Raw Data (CSV):\n",
        "emp_id,name,dept,salary,bonus\n",
        "1,Arjun,IT,75000,5000\n",
        "2,Kavya,HR,62000,\n",
        "3,Sneha,Finance,68000,4000\n",
        "4,Ramesh,Sales,58000,\n",
        "Tasks:\n",
        "Load CSV with inferred schema.\n",
        "Fill null bonuses with 2000 .\n",
        "Create total_ctc = salary + bonus .\n",
        "Filter employees with total_ctc > 65000 .\n",
        "Save result in:\n",
        "\n",
        "JSON format.\n",
        "Parquet format partitioned by department."
      ],
      "metadata": {
        "id": "bFrRtXN5owWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_data = \"\"\"emp_id,name,dept,salary,bonus\n",
        "1,Arjun,IT,75000,5000\n",
        "2,Kavya,HR,62000,\n",
        "3,Sneha,Finance,68000,4000\n",
        "4,Ramesh,Sales,58000,\n",
        "\"\"\"\n",
        "\n",
        "with open(\"employees_raw.csv\", \"w\") as f:\n",
        "    f.write(csv_data)\n"
      ],
      "metadata": {
        "id": "Gtz4ItjrogPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emp_df = spark.read.csv(\"employees_raw.csv\", header=True, inferSchema=True)\n",
        "emp_df.printSchema()\n",
        "emp_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWxZJKF6ogSh",
        "outputId": "7fc78fb0-fd10-4ce7-90c4-1f24713ca1ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- emp_id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- dept: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            " |-- bonus: integer (nullable = true)\n",
            "\n",
            "+------+------+-------+------+-----+\n",
            "|emp_id|  name|   dept|salary|bonus|\n",
            "+------+------+-------+------+-----+\n",
            "|     1| Arjun|     IT| 75000| 5000|\n",
            "|     2| Kavya|     HR| 62000| NULL|\n",
            "|     3| Sneha|Finance| 68000| 4000|\n",
            "|     4|Ramesh|  Sales| 58000| NULL|\n",
            "+------+------+-------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp_df_filled = emp_df.fillna({\"bonus\": 2000})\n"
      ],
      "metadata": {
        "id": "rE1jxDqKogU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "emp_df_ctc = emp_df_filled.withColumn(\"total_ctc\", col(\"salary\") + col(\"bonus\"))\n",
        "emp_df_ctc.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojav0nDuogXt",
        "outputId": "f710f7c1-8474-4442-c35e-b1b19f3ae0e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+------+-----+---------+\n",
            "|emp_id|  name|   dept|salary|bonus|total_ctc|\n",
            "+------+------+-------+------+-----+---------+\n",
            "|     1| Arjun|     IT| 75000| 5000|    80000|\n",
            "|     2| Kavya|     HR| 62000| 2000|    64000|\n",
            "|     3| Sneha|Finance| 68000| 4000|    72000|\n",
            "|     4|Ramesh|  Sales| 58000| 2000|    60000|\n",
            "+------+------+-------+------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df = emp_df_ctc.filter(col(\"total_ctc\") > 65000)\n",
        "filtered_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DO41COVo9Oa",
        "outputId": "2f7a8c3e-8870-47de-bfc7-431b7f2fb4dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+------+-----+---------+\n",
            "|emp_id| name|   dept|salary|bonus|total_ctc|\n",
            "+------+-----+-------+------+-----+---------+\n",
            "|     1|Arjun|     IT| 75000| 5000|    80000|\n",
            "|     3|Sneha|Finance| 68000| 4000|    72000|\n",
            "+------+-----+-------+------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df.write.mode(\"overwrite\").json(\"output/employees_json/\")\n",
        "filtered_df.write.mode(\"overwrite\").partitionBy(\"dept\").parquet(\"output/employees_parquet/\")\n"
      ],
      "metadata": {
        "id": "XZbdB8kTo9Rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BNoVQRE1o9U-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pcl5g93Eo9Xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vib8TUm2o9af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PGwDL9hwo9gE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fz3rwAuGogaX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
