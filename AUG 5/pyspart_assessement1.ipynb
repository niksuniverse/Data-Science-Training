{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. PySpark Setup & Initialization"
      ],
      "metadata": {
        "id": "Z-1HuJ_hzWLv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzJDxvYsymh3"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"BotCampus Intermediate Session\") \\\n",
        ".master(\"local[*]\") \\\n",
        ".getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"Ananya\", \"Bangalore\", 24),\n",
        "(\"Ravi\", \"Hyderabad\", 28),\n",
        "(\"Kavya\", \"Delhi\", 22),\n",
        "(\"Meena\", \"Chennai\", 25)]\n",
        "columns = [\"name\", \"city\", \"age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EM6CqGYGzZiN",
        "outputId": "963a2d11-f5cf-4736-cb2e-73a745916321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+---+\n",
            "|  name|     city|age|\n",
            "+------+---------+---+\n",
            "|Ananya|Bangalore| 24|\n",
            "|  Ravi|Hyderabad| 28|\n",
            "| Kavya|    Delhi| 22|\n",
            "| Meena|  Chennai| 25|\n",
            "+------+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDD and Transformations"
      ],
      "metadata": {
        "id": "jPuFtkMUzfkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feedback = spark.sparkContext.parallelize([\n",
        "\"Ravi from Bangalore loved the mobile app\",\n",
        "\"Meena from Delhi reported poor response time\",\n",
        "\"Ajay from Pune liked the delivery speed\",\n",
        "\"Ananya from Hyderabad had an issue with UI\",\n",
        "\"Rohit from Mumbai gave positive feedback\"\n",
        "])"
      ],
      "metadata": {
        "id": "oAXdEnUBzc08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tasks:\n",
        "\n",
        "Count total number of words.\n",
        "\n",
        "Find top 3 most common words.\n",
        "\n",
        "Remove stop words ( from , with , the , etc.).\n",
        "\n",
        "Create a dictionary of word → count."
      ],
      "metadata": {
        "id": "37clEffIzjNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_of_words=feedback.flatMap(lambda x: x.split(\" \")).count()\n",
        "print(no_of_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoLz2ZjozikT",
        "outputId": "08ef8a10-36d7-407d-bca5-e781f953d54d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_comm=feedback.flatMap(lambda x: x.split(\" \")).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y).takeOrdered(3, key = lambda x: -x[1])\n",
        "print(top_comm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-3_MpJGzzq4",
        "outputId": "2426f6c2-7e57-4650-e088-380858dc3538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('from', 5), ('the', 2), ('loved', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "filtered_word_counts = feedback.flatMap(lambda line: line.translate(str.maketrans('', '', string.punctuation)).lower().split()).filter(lambda word: word not in stop_words).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "print(\"Word counts after removing stop words:\", filtered_word_counts.collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMIq47Apzztg",
        "outputId": "a1330f95-50cd-4779-f99f-3bd58e397be1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word counts after removing stop words: [('loved', 1), ('app', 1), ('poor', 1), ('response', 1), ('liked', 1), ('speed', 1), ('ananya', 1), ('issue', 1), ('rohit', 1), ('mumbai', 1), ('positive', 1), ('feedback', 1), ('ravi', 1), ('bangalore', 1), ('mobile', 1), ('meena', 1), ('delhi', 1), ('reported', 1), ('time', 1), ('ajay', 1), ('pune', 1), ('delivery', 1), ('hyderabad', 1), ('ui', 1), ('gave', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_count=filtered_word_counts.collectAsMap()\n"
      ],
      "metadata": {
        "id": "IOMr2MsQzzwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word, count in word_count.items():\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZ-R82tPzzyS",
        "outputId": "be009d02-9317-40b1-c95d-909092aede2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loved: 1\n",
            "app: 1\n",
            "poor: 1\n",
            "response: 1\n",
            "liked: 1\n",
            "speed: 1\n",
            "ananya: 1\n",
            "issue: 1\n",
            "rohit: 1\n",
            "mumbai: 1\n",
            "positive: 1\n",
            "feedback: 1\n",
            "ravi: 1\n",
            "bangalore: 1\n",
            "mobile: 1\n",
            "meena: 1\n",
            "delhi: 1\n",
            "reported: 1\n",
            "time: 1\n",
            "ajay: 1\n",
            "pune: 1\n",
            "delivery: 1\n",
            "hyderabad: 1\n",
            "ui: 1\n",
            "gave: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. DataFrames – Transformations Tasks:\n",
        "Add grade column ( >=90 → A, 80-89 → B, 70-79 → C, else D).\n",
        "\n",
        "Group by subject, find average score.\n",
        "\n",
        "Use when and otherwise to classify subject difficulty ( Math/Science =\n",
        "Difficult).\n",
        "\n",
        "Rank students per subject using Window function.\n",
        "\n",
        "Apply UDF to format names (e.g., make all uppercase)."
      ],
      "metadata": {
        "id": "8S3gsad51-YE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = [\n",
        "(\"Ravi\", \"Math\", 88),\n",
        "(\"Ananya\", \"Science\", 92),\n",
        "(\"Kavya\", \"English\", 79),\n",
        "(\"Ravi\", \"English\", 67),\n",
        "(\"Neha\", \"Math\", 94),\n",
        "\n",
        "(\"Meena\", \"Science\", 85)\n",
        "]\n",
        "columns = [\"name\", \"subject\", \"score\"]\n",
        "df_scores = spark.createDataFrame(scores, columns)"
      ],
      "metadata": {
        "id": "zrPoy7uY197g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)Add grade column ( >=90 → A, 80-89 → B, 70-79 → C, else D)."
      ],
      "metadata": {
        "id": "SnV8KpPg3DYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "df_scores = df_scores.withColumn(\"grade\",\n",
        "                                 F.when(df_scores.score >= 90, \"A\")\n",
        "                                 .when(df_scores.score >= 80, \"B\")\n",
        "                                 .when(df_scores.score >= 70, \"C\")\n",
        "                                 .otherwise(\"D\"))\n",
        "df_scores.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkx2xL0k1-wB",
        "outputId": "f5c8c98c-a0d0-4330-b629-f1d08f180af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+\n",
            "|  name|subject|score|grade|\n",
            "+------+-------+-----+-----+\n",
            "|  Ravi|   Math|   88|    B|\n",
            "|Ananya|Science|   92|    A|\n",
            "| Kavya|English|   79|    C|\n",
            "|  Ravi|English|   67|    D|\n",
            "|  Neha|   Math|   94|    A|\n",
            "| Meena|Science|   85|    B|\n",
            "+------+-------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Task 2: Group by subject, find average score"
      ],
      "metadata": {
        "id": "_5Udpgwq3JQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "avg_scores = df_scores.groupBy(\"subject\").agg(F.avg(\"score\").alias(\"avg_score\"))\n",
        "avg_scores.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7Dq4VUG1-zU",
        "outputId": "9670ff18-d9c3-4b26-9aa7-0bb5ac1863d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+\n",
            "|subject|avg_score|\n",
            "+-------+---------+\n",
            "|Science|     88.5|\n",
            "|   Math|     91.0|\n",
            "|English|     73.0|\n",
            "+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Task 3: Use when and otherwise to classify subject difficulty"
      ],
      "metadata": {
        "id": "YT5u_3Ln3Njo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_scores = df_scores.withColumn(\"difficulty\",\n",
        "                                 F.when(df_scores.subject.isin([\"Math\", \"Science\"]), \"Difficult\")\n",
        "                                 .otherwise(\"Easy\"))\n",
        "\n",
        "df_scores.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OujojgSh1-13",
        "outputId": "ec9eb566-d6ba-453a-d812-c9370dd41922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+----------+\n",
            "|  name|subject|score|grade|difficulty|\n",
            "+------+-------+-----+-----+----------+\n",
            "|  Ravi|   Math|   88|    B| Difficult|\n",
            "|Ananya|Science|   92|    A| Difficult|\n",
            "| Kavya|English|   79|    C|      Easy|\n",
            "|  Ravi|English|   67|    D|      Easy|\n",
            "|  Neha|   Math|   94|    A| Difficult|\n",
            "| Meena|Science|   85|    B| Difficult|\n",
            "+------+-------+-----+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Task 4: Rank students per subject using Window function\n"
      ],
      "metadata": {
        "id": "ge0f44s23cfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "window = Window.partitionBy(\"subject\").orderBy(F.col(\"score\").desc())\n",
        "df_scores = df_scores.withColumn(\"rank\", F.rank().over(window))\n",
        "df_scores.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQoFpCwA2QFN",
        "outputId": "a1e74c12-851e-4600-87a8-933b8714c4e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+----------+----+\n",
            "|  name|subject|score|grade|difficulty|rank|\n",
            "+------+-------+-----+-----+----------+----+\n",
            "| Kavya|English|   79|    C|      Easy|   1|\n",
            "|  Ravi|English|   67|    D|      Easy|   2|\n",
            "|  Neha|   Math|   94|    A| Difficult|   1|\n",
            "|  Ravi|   Math|   88|    B| Difficult|   2|\n",
            "|Ananya|Science|   92|    A| Difficult|   1|\n",
            "| Meena|Science|   85|    B| Difficult|   2|\n",
            "+------+-------+-----+-----+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Task 5: Apply UDF to format names"
      ],
      "metadata": {
        "id": "z7vAd0nq4CBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "format_name = F.udf(lambda x: x.upper())\n",
        "df_scores = df_scores.withColumn(\"formatted_name\", format_name(df_scores.name))\n",
        "\n",
        "df_scores.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMhnmAZV2QMb",
        "outputId": "6ad3b48c-5c3c-40bd-cb37-2208cc71b682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+----------+----+--------------+\n",
            "|  name|subject|score|grade|difficulty|rank|formatted_name|\n",
            "+------+-------+-----+-----+----------+----+--------------+\n",
            "| Kavya|English|   79|    C|      Easy|   1|         KAVYA|\n",
            "|  Ravi|English|   67|    D|      Easy|   2|          RAVI|\n",
            "|  Neha|   Math|   94|    A| Difficult|   1|          NEHA|\n",
            "|  Ravi|   Math|   88|    B| Difficult|   2|          RAVI|\n",
            "|Ananya|Science|   92|    A| Difficult|   1|        ANANYA|\n",
            "| Meena|Science|   85|    B| Difficult|   2|         MEENA|\n",
            "+------+-------+-----+-----+----------+----+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tasks:\n",
        "Load both datasets into PySpark.\n",
        "\n",
        "Print schema and infer nested structure.\n",
        "\n",
        "Flatten the JSON (use explode , select , alias ).\n",
        "\n",
        "Convert both to Parquet and write to /tmp/output"
      ],
      "metadata": {
        "id": "PdOCQ_F14J5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Ingest CSV and JSON\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "8jfzK8xp4bgf"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load JSON file\n",
        "df_json = spark.read.option(\"multiLine\", \"True\").json(\"employee_nested.json\")\n",
        "df_json.printSchema()\n",
        "\n",
        "# Check if the dataframe contains only the _corrupt_record column\n",
        "if \"_corrupt_record\" in df_json.columns and len(df_json.columns) == 1:\n",
        "    print(\"JSON file could not be parsed correctly. Showing corrupt records:\")\n",
        "    df_json.show(truncate=False)\n",
        "else:\n",
        "    df_json.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxyIRe3t5eJ-",
        "outputId": "d0f1b0b9-ae99-4fe6-c8eb-587a6e6e8593"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- address: struct (nullable = true)\n",
            " |    |-- city: string (nullable = true)\n",
            " |    |-- pincode: long (nullable = true)\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- skills: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+----------------+---+-----+---------------+\n",
            "|         address| id| name|         skills|\n",
            "+----------------+---+-----+---------------+\n",
            "|{Mumbai, 400001}|101|Sneha|[Python, Spark]|\n",
            "+----------------+---+-----+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the JSON\n",
        "df_json_flattened = df_json.select(\"id\", \"name\", F.col(\"address.city\").alias(\"city\"), F.col(\"address.pincode\").alias(\"pincode\"), F.explode(\"skills\").alias(\"skill\"))\n",
        "df_json_flattened.printSchema()\n",
        "df_json_flattened.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qHu0ugh5cVS",
        "outputId": "447867db-dd41-4cee-e00c-ec6738dfd5f0"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- pincode: long (nullable = true)\n",
            " |-- skill: string (nullable = true)\n",
            "\n",
            "+---+-----+------+-------+------+\n",
            "| id| name|  city|pincode| skill|\n",
            "+---+-----+------+-------+------+\n",
            "|101|Sneha|Mumbai| 400001|Python|\n",
            "|101|Sneha|Mumbai| 400001| Spark|\n",
            "+---+-----+------+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85a2628f",
        "outputId": "a7e3f190-264e-47f0-e059-6c1a4f626452"
      },
      "source": [
        "# Load CSV file\n",
        "df_csv = spark.read.csv(\"students.csv\", header=True, inferSchema=True)\n",
        "df_csv.show()\n",
        "df_csv.printSchema()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+---------+------+\n",
            "| id| name|department|     city|salary|\n",
            "+---+-----+----------+---------+------+\n",
            "|  1| Amit|        IT|Bangalore| 78000|\n",
            "|  2|Kavya|        HR|  Chennai| 62000|\n",
            "|  3|Arjun|   Finance|Hyderabad| 55000|\n",
            "+---+-----+----------+---------+------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Spark SQL – Temp Views & Queries\n",
        "\n",
        "Exercise 5.1 Create view from exam scores and run:\n",
        "\n",
        "-- a) Top scorer per subject\n",
        "\n",
        "-- b) Count of students per grade\n",
        "\n",
        "-- c) Students with multiple subjects\n",
        "\n",
        "-- d) Subjects with average score above 85\n",
        "\n",
        "Exercise 5.2 Create another DataFrame attendance(name, days_present) and:\n",
        "\n",
        "Join with scores\n",
        "Calculate attendance-adjusted grade:\n",
        "If days_present < 20 → downgrade grade by one level"
      ],
      "metadata": {
        "id": "mX9crZAa6Tyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Spark SQL\").getOrCreate()\n",
        "\n",
        "scores = [\n",
        "    (\"Ravi\", \"Math\", 88),\n",
        "    (\"Ananya\", \"Science\", 92),\n",
        "    (\"Kavya\", \"English\", 79),\n",
        "    (\"Ravi\", \"English\", 67),\n",
        "    (\"Neha\", \"Math\", 94),\n",
        "    (\"Meena\", \"Science\", 85)\n",
        "]\n",
        "columns = [\"name\", \"subject\", \"score\"]\n",
        "df_scores = spark.createDataFrame(scores, columns)\n",
        "\n"
      ],
      "metadata": {
        "id": "M8jejEdg4bmL"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create view\n",
        "df_scores.createOrReplaceTempView(\"exam_scores\")\n",
        "\n",
        "# Exercise 5.1\n",
        "# a) Top scorer per subject\n",
        "spark.sql(\"\"\"\n",
        "    SELECT subject, name, score\n",
        "    FROM (\n",
        "        SELECT subject, name, score,\n",
        "        ROW_NUMBER() OVER (PARTITION BY subject ORDER BY score DESC) AS rank\n",
        "        FROM exam_scores\n",
        "    ) AS subquery\n",
        "    WHERE rank = 1\n",
        "\"\"\").show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BWe7blXv4bpS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a1895b5-3570-4d9f-8c5c-df7923cfa084"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+-----+\n",
            "|subject|  name|score|\n",
            "+-------+------+-----+\n",
            "|English| Kavya|   79|\n",
            "|   Math|  Neha|   94|\n",
            "|Science|Ananya|   92|\n",
            "+-------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# b) Count of students per grade\n",
        "df_scores = df_scores.withColumn(\"grade\",\n",
        "                                 F.when(df_scores.score >= 90, \"A\")\n",
        "                                 .when(df_scores.score >= 80, \"B\")\n",
        "                                 .when(df_scores.score >= 70, \"C\")\n",
        "                                 .otherwise(\"D\"))\n",
        "df_scores.createOrReplaceTempView(\"exam_scores\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT grade, COUNT(*) AS count\n",
        "    FROM exam_scores\n",
        "    GROUP BY grade\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "id": "EAF3NBbG4br4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da207c63-d9f3-4b90-999c-d592cf846126"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|grade|count|\n",
            "+-----+-----+\n",
            "|    B|    2|\n",
            "|    C|    1|\n",
            "|    A|    2|\n",
            "|    D|    1|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# c) Students with multiple subjects\n",
        "spark.sql(\"\"\"\n",
        "    SELECT name, COUNT(DISTINCT subject) AS subject_count\n",
        "    FROM exam_scores\n",
        "    GROUP BY name\n",
        "    HAVING subject_count > 1\n",
        "\"\"\").show()\n",
        "\n"
      ],
      "metadata": {
        "id": "f-nZKfpm4bxm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47eaba29-e994-44aa-f271-7db12632c3fc"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------------+\n",
            "|name|subject_count|\n",
            "+----+-------------+\n",
            "|Ravi|            2|\n",
            "+----+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# d) Subjects with average score above 85\n",
        "spark.sql(\"\"\"\n",
        "    SELECT subject, AVG(score) AS avg_score\n",
        "    FROM exam_scores\n",
        "    GROUP BY subject\n",
        "    HAVING AVG(score) > 85\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "id": "ojsc-VJm4b1B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49e40b0b-917f-41b4-bbe9-ee46e956069d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+\n",
            "|subject|avg_score|\n",
            "+-------+---------+\n",
            "|Science|     88.5|\n",
            "|   Math|     91.0|\n",
            "+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 5.2\n",
        "attendance = [\n",
        "    (\"Ravi\", 25),\n",
        "    (\"Ananya\", 18),\n",
        "    (\"Kavya\", 22),\n",
        "    (\"Neha\", 20),\n",
        "    (\"Meena\", 15)\n",
        "]\n",
        "columns = [\"name\", \"days_present\"]\n",
        "df_attendance = spark.createDataFrame(attendance, columns)\n",
        "\n",
        "# Join with scores\n",
        "df_joined = df_scores.join(df_attendance, \"name\")\n",
        "\n",
        "# Calculate attendance-adjusted grade\n",
        "df_joined = df_joined.withColumn(\"adjusted_grade\",\n",
        "                                  F.when(df_joined.days_present < 20, F.when(df_joined.grade == \"A\", \"B\")\n",
        "                                         .when(df_joined.grade == \"B\", \"C\")\n",
        "                                         .when(df_joined.grade == \"C\", \"D\")\n",
        "                                         .otherwise(\"D\"))\n",
        "                                  .otherwise(df_joined.grade))\n",
        "\n",
        "df_joined.show()\n"
      ],
      "metadata": {
        "id": "GNvAELyq4b4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "774d2e7e-5a9a-4863-9b6c-6864fc2e6ad8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+------------+--------------+\n",
            "|  name|subject|score|grade|days_present|adjusted_grade|\n",
            "+------+-------+-----+-----+------------+--------------+\n",
            "|Ananya|Science|   92|    A|          18|             B|\n",
            "| Kavya|English|   79|    C|          22|             C|\n",
            "| Meena|Science|   85|    B|          15|             C|\n",
            "|  Neha|   Math|   94|    A|          20|             A|\n",
            "|  Ravi|   Math|   88|    B|          25|             B|\n",
            "|  Ravi|English|   67|    D|          25|             D|\n",
            "+------+-------+-----+-----+------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Partitioned Load (Full + Incremental)\n",
        "\n",
        "Initial Load:\n",
        "\n",
        "df_scores.write.partitionBy(\"subject\").parquet(\"/tmp/scores/\")\n",
        "\n",
        "Incremental Load:\n",
        "\n",
        "incremental = [(\"Meena\", \"Math\", 93)]\n",
        "\n",
        "df_inc = spark.createDataFrame(incremental, columns)\n",
        "\n",
        "df_inc.write.mode(\"append\").partitionBy(\"subject\").parquet(\"/tmp/scores/\")\n",
        "\n",
        "Task:\n",
        "\n",
        "List all folders inside /tmp/scores/\n",
        "\n",
        "Read only Math partition and display all entries."
      ],
      "metadata": {
        "id": "DJBs7sb27woY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Partitioned Load\").getOrCreate()\n",
        "scores = [\n",
        "    (\"Ravi\", \"Math\", 88),\n",
        "    (\"Ananya\", \"Science\", 92),\n",
        "    (\"Kavya\", \"English\", 79),\n",
        "    (\"Ravi\", \"English\", 67),\n",
        "    (\"Neha\", \"Math\", 94),\n",
        "    (\"Meena\", \"Science\", 85)\n",
        "]\n",
        "columns = [\"name\", \"subject\", \"score\"]\n",
        "df_scores = spark.createDataFrame(scores, columns)\n",
        "df_scores.write.partitionBy(\"subject\").parquet(\"/tmp/scores/\")"
      ],
      "metadata": {
        "id": "t6aG-Jxf7xux"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Incremental Load\n",
        "incremental = [(\"Meena\", \"Math\", 93)]\n",
        "df_inc = spark.createDataFrame(incremental, columns)\n",
        "df_inc.write.mode(\"append\").partitionBy(\"subject\").parquet(\"/tmp/scores/\")"
      ],
      "metadata": {
        "id": "WJxPs3R47x_K"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: List all folders inside /tmp/scores/\n",
        "import subprocess\n",
        "folders = subprocess.check_output([\"ls\", \"/tmp/scores/\"]).decode(\"utf-8\").splitlines()\n",
        "print(folders)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFlXsFeC7yBa",
        "outputId": "c8900d73-17c0-43cb-827e-bf7c4e072869"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['subject=English', 'subject=Math', 'subject=Science', '_SUCCESS']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Read only Math partition and display all entries\n",
        "df_math = spark.read.parquet(\"/tmp/scores/subject=Math\")\n",
        "df_math.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jRPBtcJ7yD8",
        "outputId": "fcb43265-45c1-4668-deb3-5f1d91c9fa47"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "| name|score|\n",
            "+-----+-----+\n",
            "|Meena|   93|\n",
            "| Neha|   94|\n",
            "| Ravi|   88|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_scores = spark.read.parquet(\"/tmp/scores/\")\n",
        "df_scores.filter(df_scores.subject == \"Math\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3OHo-lq7yG5",
        "outputId": "1040cf39-2103-4e82-9ea3-d1b2af2e977b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-------+\n",
            "| name|score|subject|\n",
            "+-----+-----+-------+\n",
            "|Meena|   93|   Math|\n",
            "| Neha|   94|   Math|\n",
            "| Ravi|   88|   Math|\n",
            "+-----+-----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. ETL: Clean, Transform, Load\n",
        "Raw CSV:\n",
        "\n",
        "emp_id,name,dept,salary,bonus\n",
        "1,Arjun,IT,78000,5000\n",
        "2,Kavya,HR,62000,\n",
        "3,Sneha,Finance,55000,3000\n",
        "\n",
        "Tasks:\n",
        "\n",
        "Load data with header.\n",
        "\n",
        "Fill missing bonus with 2000.\n",
        "\n",
        "Calculate total_ctc = salary + bonus .\n",
        "\n",
        "Filter where total_ctc > 60,000.\n",
        "\n",
        "Save final DataFrame to Parquet and JSON.\n"
      ],
      "metadata": {
        "id": "rFPO4P1J8gec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "spark = SparkSession.builder.appName(\"ETL\").getOrCreate()\n",
        "\n",
        "df = spark.read.csv(\"emp_data.csv\", header=True, inferSchema=True)\n",
        "\n",
        "df = df.withColumn(\"bonus\", F.coalesce(df.bonus, F.lit(2000)))\n",
        "\n",
        "\n",
        "df = df.withColumn(\"total_ctc\", df.salary + df.bonus)\n",
        "\n",
        "df = df.filter(df.total_ctc > 60000)\n",
        "\n",
        "df.write.parquet(\"/tmp/emp_data_parquet\", mode=\"overwrite\")\n",
        "df.write.json(\"/tmp/emp_data_json\", mode=\"overwrite\")\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcF2y7Kl7x6C",
        "outputId": "a6cd4531-490e-445f-e960-ea6cd22d0f0c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+----+------+-----+---------+\n",
            "|emp_id| name|dept|salary|bonus|total_ctc|\n",
            "+------+-----+----+------+-----+---------+\n",
            "|     1|Arjun|  IT| 78000| 5000|    83000|\n",
            "|     2|Kavya|  HR| 62000| 2000|    64000|\n",
            "+------+-----+----+------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}