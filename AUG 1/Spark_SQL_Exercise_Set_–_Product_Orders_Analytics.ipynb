{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Preparation Instructions\n",
        "\n",
        "1. Create a PySpark DataFrame with the following schema:\n",
        "OrderID (int)\n",
        "CustomerName (string)\n",
        "Product (string)\n",
        "Category (string)\n",
        "Quantity (int)\n",
        "UnitPrice (int)\n",
        "OrderDate (string in YYYY-MM-DD format)\n",
        "\n",
        "2. Sample at least 12 rows across multiple categories:\n",
        "\"Electronics\" , \"Clothing\" , \"Furniture\" , \"Books\"\n",
        "\n",
        "3. Create:\n",
        "A local temporary view: \"orders_local\"\n",
        "A global temporary view: \"orders_global\""
      ],
      "metadata": {
        "id": "QDks3QvxTW4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "data = [\n",
        "    Row(OrderID=1, CustomerName=\"Ahana\", Product=\"Laptop\", Category=\"Electronics\", Quantity=2, UnitPrice=1000, OrderDate=\"2023-01-10\"),\n",
        "    Row(OrderID=2, CustomerName=\"Brindha\", Product=\"Smartphone\", Category=\"Electronics\", Quantity=1, UnitPrice=800, OrderDate=\"2023-02-15\"),\n",
        "    Row(OrderID=3, CustomerName=\"Sindhana\", Product=\"T-Shirt\", Category=\"Clothing\", Quantity=5, UnitPrice=20, OrderDate=\"2023-01-05\"),\n",
        "    Row(OrderID=4, CustomerName=\"Zara\", Product=\"Sofa\", Category=\"Furniture\", Quantity=1, UnitPrice=15000, OrderDate=\"2023-03-20\"),\n",
        "    Row(OrderID=5, CustomerName=\"Elakkiya\", Product=\"Bookshelf\", Category=\"Furniture\", Quantity=2, UnitPrice=3000, OrderDate=\"2023-01-30\"),\n",
        "    Row(OrderID=6, CustomerName=\"Ferose\", Product=\"Novel\", Category=\"Books\", Quantity=3, UnitPrice=15, OrderDate=\"2023-04-01\"),\n",
        "    Row(OrderID=7, CustomerName=\"Goutham\", Product=\"Tablet\", Category=\"Electronics\", Quantity=3, UnitPrice=400, OrderDate=\"2023-03-25\"),\n",
        "    Row(OrderID=8, CustomerName=\"Harish\", Product=\"Jeans\", Category=\"Clothing\", Quantity=2, UnitPrice=40, OrderDate=\"2023-02-20\"),\n",
        "    Row(OrderID=9, CustomerName=\"Sathya\", Product=\"Notebook\", Category=\"Books\", Quantity=4, UnitPrice=10, OrderDate=\"2023-01-12\"),\n",
        "    Row(OrderID=10, CustomerName=\"Jackie\", Product=\"Chair\", Category=\"Furniture\", Quantity=5, UnitPrice=2500, OrderDate=\"2023-01-08\"),\n",
        "    Row(OrderID=11, CustomerName=\"Krishna\", Product=\"Dress\", Category=\"Clothing\", Quantity=1, UnitPrice=60, OrderDate=\"2023-05-18\"),\n",
        "    Row(OrderID=12, CustomerName=\"Nikitha\", Product=\"Camera\", Category=\"Electronics\", Quantity=2, UnitPrice=1200, OrderDate=\"2023-01-03\"),\n",
        "    Row(OrderID=12, CustomerName=\"Nikitha\", Product=\"Dress\", Category=\"Clothig\", Quantity=2, UnitPrice=1100, OrderDate=\"2023-02-03\"),\n",
        "]\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "df.createOrReplaceTempView(\"orders_local\")\n",
        "df.createOrReplaceGlobalTempView(\"orders_global\")\n"
      ],
      "metadata": {
        "id": "whYsNPPcU_O2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part A: Local View – orders_local\n",
        "\n",
        "1. List all orders placed for \"Electronics\" with a Quantity of 2 or more.\n",
        "2. Calculate TotalAmount (Quantity × UnitPrice) for each order.\n",
        "3. Show the total number of orders per Category .\n",
        "4. List orders placed in \"January 2023\" only.\n",
        "5. Show the average UnitPrice per category.\n",
        "6. Find the order with the highest total amount.\n",
        "7. Drop the local view and try querying it again."
      ],
      "metadata": {
        "id": "5H7P6U3YT0jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select * from orders_local where Category = 'Electronics' and Quantity >= 2\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUaIMaFsVW5c",
        "outputId": "45f54066-cbfc-4fd4-a1f3-965d57834f52"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+-------+-----------+--------+---------+----------+\n",
            "|OrderID|CustomerName|Product|   Category|Quantity|UnitPrice| OrderDate|\n",
            "+-------+------------+-------+-----------+--------+---------+----------+\n",
            "|      1|       Ahana| Laptop|Electronics|       2|     1000|2023-01-10|\n",
            "|      7|     Goutham| Tablet|Electronics|       3|      400|2023-03-25|\n",
            "|     12|     Nikitha| Camera|Electronics|       2|     1200|2023-01-03|\n",
            "+-------+------------+-------+-----------+--------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. calculate TotalAmount (Quantity × UnitPrice) for each order.\n",
        "spark.sql(\"select orderId,CustomerName,Product,Category,OrderDate,Quantity*UnitPrice as TotalAmount from orders_local\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMNFxuwgXVsb",
        "outputId": "0557fef5-4b66-4e31-8716-1c428656fb61"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+----------+-----------+----------+-----------+\n",
            "|orderId|CustomerName|   Product|   Category| OrderDate|TotalAmount|\n",
            "+-------+------------+----------+-----------+----------+-----------+\n",
            "|      1|       Ahana|    Laptop|Electronics|2023-01-10|       2000|\n",
            "|      2|     Brindha|Smartphone|Electronics|2023-02-15|        800|\n",
            "|      3|    Sindhana|   T-Shirt|   Clothing|2023-01-05|        100|\n",
            "|      4|        Zara|      Sofa|  Furniture|2023-03-20|      15000|\n",
            "|      5|    Elakkiya| Bookshelf|  Furniture|2023-01-30|       6000|\n",
            "|      6|      Ferose|     Novel|      Books|2023-04-01|         45|\n",
            "|      7|     Goutham|    Tablet|Electronics|2023-03-25|       1200|\n",
            "|      8|      Harish|     Jeans|   Clothing|2023-02-20|         80|\n",
            "|      9|      Sathya|  Notebook|      Books|2023-01-12|         40|\n",
            "|     10|      Jackie|     Chair|  Furniture|2023-01-08|      12500|\n",
            "|     11|     Krishna|     Dress|   Clothing|2023-05-18|         60|\n",
            "|     12|     Nikitha|    Camera|Electronics|2023-01-03|       2400|\n",
            "+-------+------------+----------+-----------+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Show the total number of orders per Category\n",
        "spark.sql(\"select Category,count(*) as total_count from orders_local group by Category\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yxj4WxoTXVpx",
        "outputId": "4c8e2e75-73d8-4e7f-9b57-355d3fda00a3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+\n",
            "|   Category|total_count|\n",
            "+-----------+-----------+\n",
            "|Electronics|          4|\n",
            "|   Clothing|          3|\n",
            "|      Books|          2|\n",
            "|  Furniture|          3|\n",
            "+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. List orders placed in \"January 2023\" only\n",
        "spark.sql(\"select * from orders_local where OrderDate like '2023-01%'\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sr4mBZlYXVnB",
        "outputId": "38d44227-02cc-40db-df08-47a940b2377e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+---------+-----------+--------+---------+----------+\n",
            "|OrderID|CustomerName|  Product|   Category|Quantity|UnitPrice| OrderDate|\n",
            "+-------+------------+---------+-----------+--------+---------+----------+\n",
            "|      1|       Ahana|   Laptop|Electronics|       2|     1000|2023-01-10|\n",
            "|      3|    Sindhana|  T-Shirt|   Clothing|       5|       20|2023-01-05|\n",
            "|      5|    Elakkiya|Bookshelf|  Furniture|       2|     3000|2023-01-30|\n",
            "|      9|      Sathya| Notebook|      Books|       4|       10|2023-01-12|\n",
            "|     10|      Jackie|    Chair|  Furniture|       5|     2500|2023-01-08|\n",
            "|     12|     Nikitha|   Camera|Electronics|       2|     1200|2023-01-03|\n",
            "+-------+------------+---------+-----------+--------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Show the average UnitPrice per category\n",
        "spark.sql(\"select Category,avg(UnitPrice) as avg_price from orders_local group by Category\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf3xKJs0XVkh",
        "outputId": "d50a4530-660b-43b5-ee99-5e53b4bba64a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------------+\n",
            "|   Category|        avg_price|\n",
            "+-----------+-----------------+\n",
            "|Electronics|            850.0|\n",
            "|   Clothing|             40.0|\n",
            "|      Books|             12.5|\n",
            "|  Furniture|6833.333333333333|\n",
            "+-----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Find the order with the highest total amount\n",
        "spark.sql(\"select orderId,CustomerName,Product,Category,OrderDate,Quantity*UnitPrice as TotalAmount from orders_local order by TotalAmount desc limit 1\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9nseIWUXViV",
        "outputId": "6a06b10b-6ce5-4659-a8b0-f8f3cc946bd5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+-------+---------+----------+-----------+\n",
            "|orderId|CustomerName|Product| Category| OrderDate|TotalAmount|\n",
            "+-------+------------+-------+---------+----------+-----------+\n",
            "|      4|        Zara|   Sofa|Furniture|2023-03-20|      15000|\n",
            "+-------+------------+-------+---------+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Drop the local view and try querying it again\n",
        "spark.catalog.dropTempView(\"orders_local\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VJYMwmtXVf4",
        "outputId": "94bb5c09-e668-4970-845a-73ea4958b071"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"Select * from orders_local\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "xdTFhqZcXVdL",
        "outputId": "7075e8fd-d9b8-427a-cfd5-9cd79b7bfc63"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `orders_local` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [orders_local], [], false\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3559514180.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Select * from orders_local\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                 )\n\u001b[0;32m-> 1631\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitArgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `orders_local` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [orders_local], [], false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part B: Global View – orders_global\n",
        "\n",
        "1. Display all \"Furniture\" orders with TotalAmount above\n",
        "10,000.\n",
        "2. Create a column called DiscountFlag :\n",
        "Mark \"Yes\" if Quantity > 3\n",
        "Otherwise \"No\"\n",
        "\n",
        "3. List customers who ordered more than 1 product type (Hint: use GROUP BY and\n",
        "HAVING).\n",
        "\n",
        "4. Count number of orders per month across the dataset.\n",
        "\n",
        "5. Rank all products by total quantity sold across all orders using a window\n",
        "function.\n",
        "\n",
        "6. Run a query using a new SparkSession and the global view."
      ],
      "metadata": {
        "id": "dBALG5eBZk0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "select *, Quantity * UnitPrice AS TotalAmount\n",
        "from global_temp.orders_global\n",
        "where Category = 'Furniture' and (Quantity * UnitPrice) > 10000\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "od_Fh_-1XVax",
        "outputId": "679f39df-7aa5-4024-9ce7-cb2bcdd25146"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+-------+---------+--------+---------+----------+-----------+\n",
            "|OrderID|CustomerName|Product| Category|Quantity|UnitPrice| OrderDate|TotalAmount|\n",
            "+-------+------------+-------+---------+--------+---------+----------+-----------+\n",
            "|      4|        Zara|   Sofa|Furniture|       1|    15000|2023-03-20|      15000|\n",
            "|     10|      Jackie|  Chair|Furniture|       5|     2500|2023-01-08|      12500|\n",
            "+-------+------------+-------+---------+--------+---------+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.Create a column called DiscountFlag : Mark \"Yes\" if Quantity > 3 Otherwise \"No\"\n",
        "spark.sql(\"\"\"\n",
        "select *, case when quantity > 3 then 'yes' else 'no' end as discountflag\n",
        "from global_temp.orders_global\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gv4I4JCbXVWf",
        "outputId": "9214b0d1-77dd-4eaf-eeca-74f18133d188"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+----------+-----------+--------+---------+----------+------------+\n",
            "|OrderID|CustomerName|   Product|   Category|Quantity|UnitPrice| OrderDate|discountflag|\n",
            "+-------+------------+----------+-----------+--------+---------+----------+------------+\n",
            "|      1|       Ahana|    Laptop|Electronics|       2|     1000|2023-01-10|          no|\n",
            "|      2|     Brindha|Smartphone|Electronics|       1|      800|2023-02-15|          no|\n",
            "|      3|    Sindhana|   T-Shirt|   Clothing|       5|       20|2023-01-05|         yes|\n",
            "|      4|        Zara|      Sofa|  Furniture|       1|    15000|2023-03-20|          no|\n",
            "|      5|    Elakkiya| Bookshelf|  Furniture|       2|     3000|2023-01-30|          no|\n",
            "|      6|      Ferose|     Novel|      Books|       3|       15|2023-04-01|          no|\n",
            "|      7|     Goutham|    Tablet|Electronics|       3|      400|2023-03-25|          no|\n",
            "|      8|      Harish|     Jeans|   Clothing|       2|       40|2023-02-20|          no|\n",
            "|      9|      Sathya|  Notebook|      Books|       4|       10|2023-01-12|         yes|\n",
            "|     10|      Jackie|     Chair|  Furniture|       5|     2500|2023-01-08|         yes|\n",
            "|     11|     Krishna|     Dress|   Clothing|       1|       60|2023-05-18|          no|\n",
            "|     12|     Nikitha|    Camera|Electronics|       2|     1200|2023-01-03|          no|\n",
            "+-------+------------+----------+-----------+--------+---------+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.List customers who ordered more than 1 product type (Hint: use GROUP BY and HAVING).\n",
        "spark.sql(\"\"\"\n",
        "select customername, count(distinct product) as productcount\n",
        "from global_temp.orders_global\n",
        "group by customername\n",
        "having count(distinct product) > 1\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypNwXhzna5xr",
        "outputId": "260ca00e-4ba0-49ec-f1e2-c64b8853b006"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------+\n",
            "|customername|productcount|\n",
            "+------------+------------+\n",
            "|     Nikitha|           2|\n",
            "+------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Count number of orders per month across the dataset.\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "select substring(orderdate, 1, 7) as ordermonth, count(*) as ordercount\n",
        "from global_temp.orders_global\n",
        "group by substring(orderdate, 1, 7)\n",
        "order by ordermonth\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ae4VlKd4a50o",
        "outputId": "09e4bc81-1091-4e33-945a-254f020f1311"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|ordermonth|ordercount|\n",
            "+----------+----------+\n",
            "|   2023-01|         6|\n",
            "|   2023-02|         3|\n",
            "|   2023-03|         2|\n",
            "|   2023-04|         1|\n",
            "|   2023-05|         1|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Rank all products by total quantity sold across all orders using a window function.\n",
        "from pyspark.sql.functions import sum as _sum\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank\n",
        "products_df = spark.sql(\"\"\"\n",
        "    select product, sum(quantity) as totalsold\n",
        "    from global_temp.orders_global\n",
        "    group by product\n",
        "\"\"\")\n",
        "\n",
        "windowSpec = Window.orderBy(col(\"TotalSold\").desc())\n",
        "ranked = products_df.withColumn(\"Rank\", rank().over(windowSpec))\n",
        "ranked.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXFAkjKva53v",
        "outputId": "4ac52cf6-7ec4-4605-df41-318fbb86a00d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+----+\n",
            "|   product|totalsold|Rank|\n",
            "+----------+---------+----+\n",
            "|   T-Shirt|        5|   1|\n",
            "|     Chair|        5|   1|\n",
            "|  Notebook|        4|   3|\n",
            "|     Novel|        3|   4|\n",
            "|     Dress|        3|   4|\n",
            "|    Tablet|        3|   4|\n",
            "|    Laptop|        2|   7|\n",
            "| Bookshelf|        2|   7|\n",
            "|    Camera|        2|   7|\n",
            "|     Jeans|        2|   7|\n",
            "|      Sofa|        1|  11|\n",
            "|Smartphone|        1|  11|\n",
            "+----------+---------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Run a query using a new SparkSession and the global view.\n",
        "\n",
        "new_spark = SparkSession.builder.appName(\"NewSession\").getOrCreate()\n",
        "new_spark.sql(\"select * from global_temp.orders_global where Category = 'Books'\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQiOuIVAb429",
        "outputId": "b3e36179-f7ed-4d00-fcdd-7ef4fabbf5f7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+--------+--------+--------+---------+----------+\n",
            "|OrderID|CustomerName| Product|Category|Quantity|UnitPrice| OrderDate|\n",
            "+-------+------------+--------+--------+--------+---------+----------+\n",
            "|      6|      Ferose|   Novel|   Books|       3|       15|2023-04-01|\n",
            "|      9|      Sathya|Notebook|   Books|       4|       10|2023-01-12|\n",
            "+-------+------------+--------+--------+--------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bonus Challenges"
      ],
      "metadata": {
        "id": "JtvG0KsAcPsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "books_df = df.filter(col(\"Category\") == \"Books\")\n",
        "books_df.createOrReplaceGlobalTempView(\"books_global\")\n",
        "books_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFjKU_8ha56d",
        "outputId": "2c997b6c-fbc3-4683-b845-264030ed2ade"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+--------+--------+--------+---------+----------+\n",
            "|OrderID|CustomerName| Product|Category|Quantity|UnitPrice| OrderDate|\n",
            "+-------+------------+--------+--------+--------+---------+----------+\n",
            "|      6|      Ferose|   Novel|   Books|       3|       15|2023-04-01|\n",
            "|      9|      Sathya|Notebook|   Books|       4|       10|2023-01-12|\n",
            "+-------+------------+--------+--------+--------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import row_number\n",
        "\n",
        "agg_df = df.groupBy(\"Category\", \"Product\").agg(_sum(\"Quantity\").alias(\"TotalQty\"))\n",
        "windowSpec = Window.partitionBy(\"Category\").orderBy(col(\"TotalQty\").desc())\n",
        "top_products = agg_df.withColumn(\"rank\", row_number().over(windowSpec)).filter(\"rank = 1\")\n",
        "top_products.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_M6mXcgGcUKn",
        "outputId": "08788f29-9f6a-47da-9268-c44691fa2a65"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+--------+----+\n",
            "|   Category| Product|TotalQty|rank|\n",
            "+-----------+--------+--------+----+\n",
            "|      Books|Notebook|       4|   1|\n",
            "|    Clothig|   Dress|       2|   1|\n",
            "|   Clothing| T-Shirt|       5|   1|\n",
            "|Electronics|  Tablet|       3|   1|\n",
            "|  Furniture|   Chair|       5|   1|\n",
            "+-----------+--------+--------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df = df.filter(col(\"Category\") != \"Clothing\")\n",
        "filtered_df.createOrReplaceTempView(\"filtered_orders\")\n",
        "filtered_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a22gJX6icX_k",
        "outputId": "7e3c108b-9372-4a9b-f862-fa8a9197aaca"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+----------+-----------+--------+---------+----------+\n",
            "|OrderID|CustomerName|   Product|   Category|Quantity|UnitPrice| OrderDate|\n",
            "+-------+------------+----------+-----------+--------+---------+----------+\n",
            "|      1|       Ahana|    Laptop|Electronics|       2|     1000|2023-01-10|\n",
            "|      2|     Brindha|Smartphone|Electronics|       1|      800|2023-02-15|\n",
            "|      4|        Zara|      Sofa|  Furniture|       1|    15000|2023-03-20|\n",
            "|      5|    Elakkiya| Bookshelf|  Furniture|       2|     3000|2023-01-30|\n",
            "|      6|      Ferose|     Novel|      Books|       3|       15|2023-04-01|\n",
            "|      7|     Goutham|    Tablet|Electronics|       3|      400|2023-03-25|\n",
            "|      9|      Sathya|  Notebook|      Books|       4|       10|2023-01-12|\n",
            "|     10|      Jackie|     Chair|  Furniture|       5|     2500|2023-01-08|\n",
            "|     12|     Nikitha|    Camera|Electronics|       2|     1200|2023-01-03|\n",
            "|     12|     Nikitha|     Dress|    Clothig|       2|     1100|2023-02-03|\n",
            "+-------+------------+----------+-----------+--------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}